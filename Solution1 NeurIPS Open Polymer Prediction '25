{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"},{"sourceId":11605551,"sourceType":"datasetVersion","datasetId":7279248},{"sourceId":12189904,"sourceType":"datasetVersion","datasetId":7678100},{"sourceId":12207625,"sourceType":"datasetVersion","datasetId":7690162},{"sourceId":12237259,"sourceType":"datasetVersion","datasetId":7706066},{"sourceId":12330396,"sourceType":"datasetVersion","datasetId":7709869},{"sourceId":13048231,"sourceType":"datasetVersion","datasetId":8238327},{"sourceId":247698673,"sourceType":"kernelVersion"},{"sourceId":247701857,"sourceType":"kernelVersion"},{"sourceId":251268093,"sourceType":"kernelVersion"},{"sourceId":442871,"sourceType":"modelInstanceVersion","modelInstanceId":359690,"modelId":380893}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":132.285112,"end_time":"2025-09-12T13:03:14.345395","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-12T13:01:02.060283","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp -r /kaggle/input/autogluon-package/* /kaggle/working/ > /dev/null\n!pip install -f --quiet --no-index --find-links='/kaggle/input/autogluon-package' 'autogluon.tabular-1.3.1-py3-none-any.whl' > /dev/null","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2025-09-12T13:01:06.243599Z","iopub.status.busy":"2025-09-12T13:01:06.243396Z","iopub.status.idle":"2025-09-12T13:01:48.054267Z","shell.execute_reply":"2025-09-12T13:01:48.053507Z"},"papermill":{"duration":41.820864,"end_time":"2025-09-12T13:01:48.055724","exception":false,"start_time":"2025-09-12T13:01:06.23486","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp -r /kaggle/input/scikit-package/* /kaggle/working/ > /dev/null\n!pip install -f --quiet --no-index --find-links='/kaggle/input/scikit-package' 'scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl' > /dev/null\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2025-09-12T13:01:48.073005Z","iopub.status.busy":"2025-09-12T13:01:48.072736Z","iopub.status.idle":"2025-09-12T13:01:54.531048Z","shell.execute_reply":"2025-09-12T13:01:54.530236Z"},"papermill":{"duration":6.468023,"end_time":"2025-09-12T13:01:54.532363","exception":false,"start_time":"2025-09-12T13:01:48.06434","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from autogluon.tabular import TabularDataset, TabularPredictor","metadata":{"execution":{"iopub.execute_input":"2025-09-12T13:01:54.549402Z","iopub.status.busy":"2025-09-12T13:01:54.548671Z","iopub.status.idle":"2025-09-12T13:01:58.2483Z","shell.execute_reply":"2025-09-12T13:01:58.247704Z"},"papermill":{"duration":3.709165,"end_time":"2025-09-12T13:01:58.249608","exception":false,"start_time":"2025-09-12T13:01:54.540443","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl > /dev/null","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2025-09-12T13:01:58.265653Z","iopub.status.busy":"2025-09-12T13:01:58.26533Z","iopub.status.idle":"2025-09-12T13:02:03.087804Z","shell.execute_reply":"2025-09-12T13:02:03.087005Z"},"papermill":{"duration":4.832111,"end_time":"2025-09-12T13:02:03.089581","exception":false,"start_time":"2025-09-12T13:01:58.25747","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install mordred --no-index --find-links=file:///kaggle/input/mordred-1-2-0-py3-none-any/ > /dev/null","metadata":{"execution":{"iopub.execute_input":"2025-09-12T13:02:03.107049Z","iopub.status.busy":"2025-09-12T13:02:03.106393Z","iopub.status.idle":"2025-09-12T13:02:08.025072Z","shell.execute_reply":"2025-09-12T13:02:08.024055Z"},"papermill":{"duration":4.92915,"end_time":"2025-09-12T13:02:08.027165","exception":false,"start_time":"2025-09-12T13:02:03.098015","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.execute_input":"2025-09-12T13:02:08.046453Z","iopub.status.busy":"2025-09-12T13:02:08.045649Z","iopub.status.idle":"2025-09-12T13:02:08.896482Z","shell.execute_reply":"2025-09-12T13:02:08.895462Z"},"papermill":{"duration":0.862044,"end_time":"2025-09-12T13:02:08.898061","exception":false,"start_time":"2025-09-12T13:02:08.036017","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/'","metadata":{"execution":{"iopub.execute_input":"2025-09-12T13:02:08.916188Z","iopub.status.busy":"2025-09-12T13:02:08.915565Z","iopub.status.idle":"2025-09-12T13:02:08.919178Z","shell.execute_reply":"2025-09-12T13:02:08.918647Z"},"papermill":{"duration":0.013319,"end_time":"2025-09-12T13:02:08.920211","exception":false,"start_time":"2025-09-12T13:02:08.906892","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_dfs = []","metadata":{"execution":{"iopub.execute_input":"2025-09-12T13:02:08.937848Z","iopub.status.busy":"2025-09-12T13:02:08.937614Z","iopub.status.idle":"2025-09-12T13:02:08.940754Z","shell.execute_reply":"2025-09-12T13:02:08.940226Z"},"papermill":{"duration":0.012536,"end_time":"2025-09-12T13:02:08.941725","exception":false,"start_time":"2025-09-12T13:02:08.929189","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGB","metadata":{"papermill":{"duration":0.007655,"end_time":"2025-09-12T13:02:08.957201","exception":false,"start_time":"2025-09-12T13:02:08.949546","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import glob\nimport os\nimport time\nimport random\nimport json\nimport hashlib\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\n\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, StackingRegressor\nfrom sklearn.feature_selection import VarianceThreshold, SelectFromModel\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.linear_model import LassoCV, ElasticNetCV\n\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\n\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors, MACCSkeys, rdmolops, Lipinski, Crippen\nfrom rdkit.Chem.rdMolDescriptors import CalcNumRotatableBonds\nfrom rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator, GetAtomPairGenerator, GetTopologicalTorsionGenerator\n\nfrom mordred import Calculator, descriptors as mordred_descriptors\n\nimport shap\n\nfrom transformers import AutoTokenizer, AutoModel\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\n# torchinfo is optional, only used in show_model_summary\ntry:\n    from torchinfo import summary\nexcept ImportError:\n    summary = None\n\n# Data paths\nRDKIT_AVAILABLE = True\nTARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\n# Create the NeurIPS directory if it does not exist\nos.makedirs(\"NeurIPS\", exist_ok=True)\nclass Config:\n    useAllDataForTraining = False\n    use_standard_scaler = True  # Set to True to use StandardScaler, False to skip scaling\n    # Set to True to calculate Mordred descriptors in featurization\n    use_least_important_features_all_methods = True  # Set to True to call get_least_important_features_all_methods\n    use_variance_threshold = False  # Set to True to enable VarianceThreshold feature selection\n    enable_param_tuning = False  # Set to True to enable XGB hyperparameter tuning\n    debug = False\n\n    use_descriptors = False  # Set to True to include RDKit descriptors, False to skip\n    use_mordred = True\n    # Control inclusion of AtomPair and TopologicalTorsion fingerprints\n    use_maccs_fp = False\n    use_morgan_fp = False\n    use_atom_pair_fp = False\n    use_torsion_fp = False\n    use_chemberta = False\n    chemberta_pooling = 'max'  # can be 'mean', 'max', 'cls', or 'pooler'\n    # Include MACCS keys fingerprint\n\n    search_nn = False\n    use_stacking = False\n    model_name = 'xgb'  \n    # Options: ['autogluon', xgb', 'catboost', 'lgbm', 'extratrees', 'randomforest', 'tabnet', 'hgbm', 'nn']\n\n    # Don't change\n    # Choose importance method: 'feature_importances_' or 'permutation_importance'\n    feature_importance_method = 'permutation_importance'\n    use_cross_validation = True  # Set to False to use a single split for speed\n    use_pca = False  # Set to True to enable PCA\n    pca_variance = 0.95  # Fraction of variance to keep in PCA\n    use_external_data = True  # Set to True to use external datasets\n    use_augmentation = False  # Set to True to use augment_dataset\n    add_gaussian = False  # Set to True to enable Gaussian Mixture Model-based augmentation\n    random_state = 321\n\n\n    # Number of least important features to drop\n    # Use a nested dictionary for model- and label-specific n_least_important_features\n    n_least_important_features = {\n        'xgb':     {'Tg': 20, 'FFV': 20, 'Tc': 22, 'Density': 19, 'Rg': 19},\n        'catboost':{'Tg': 15, 'FFV': 15, 'Tc': 18, 'Density': 15, 'Rg': 15},\n        'lgbm':    {'Tg': 18, 'FFV': 18, 'Tc': 20, 'Density': 17, 'Rg': 17},\n        'extratrees':{'Tg': 22, 'FFV': 15, 'Tc': 10, 'Density': 25, 'Rg': 5},\n        'randomforest':{'Tg': 21, 'FFV': 19, 'Tc': 21, 'Density': 18, 'Rg': 18},\n        'balancedrf':{'Tg': 20, 'FFV': 20, 'Tc': 20, 'Density': 20, 'Rg': 20},\n    }\n\n    # Path for permutation importance log file\n    permutation_importance_log_path = \"log/permutation_importance_log.xlsx\"\n\n    correlation_threshold_value = 0.96\n    correlation_thresholds = {\n        \"Tg\": correlation_threshold_value,\n        \"FFV\": correlation_threshold_value,\n        \"Tc\": correlation_threshold_value,\n        \"Density\": correlation_threshold_value,\n        \"Rg\": correlation_threshold_value\n    }\n\n# Create a single config instance to use everywhere\nconfig = Config()\n\nif config.debug or config.search_nn:\n    config.use_cross_validation = False\n\n# --- XGB Hyperparameter Tuning DB Utilities ---\nimport sqlite3\nimport hashlib\nimport json\n\ndef init_chemberta():\n    model_name = \"/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM\"\n    chemberta_tokenizer = AutoTokenizer.from_pretrained(model_name)\n    chemberta_model = AutoModel.from_pretrained(model_name)\n    chemberta_model.eval()\n    return chemberta_tokenizer, chemberta_model\n\ndef get_chemberta_embedding(smiles, embedding_dim=384):\n    \"\"\"\n    Returns ChemBERTa embedding for a single SMILES string.\n    Pads/truncates to embedding_dim if needed.\n    \"\"\"\n    if smiles is None or not isinstance(smiles, str) or len(smiles) == 0:\n        return np.zeros(embedding_dim)\n    try:\n        # Add pooling argument with default 'mean'\n        pooling = getattr(config, 'chemberta_pooling', 'mean')  # can be 'mean', 'max', 'cls', 'pooler'\n        chemberta_tokenizer, chemberta_model = init_chemberta()\n        inputs = chemberta_tokenizer([smiles], padding=True, return_tensors=\"pt\", truncation=True, max_length=512)\n        with torch.no_grad():\n            outputs = chemberta_model(**inputs)\n            if pooling == 'pooler' and hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n                emb = outputs.pooler_output.squeeze(0)\n            elif pooling == 'cls' and hasattr(outputs, 'last_hidden_state'):\n                emb = outputs.last_hidden_state[:, 0, :].squeeze(0)\n            elif pooling == 'max' and hasattr(outputs, 'last_hidden_state'):\n                emb = outputs.last_hidden_state.max(dim=1).values.squeeze(0)\n            elif pooling == 'mean' and hasattr(outputs, 'last_hidden_state'):\n                emb = outputs.last_hidden_state.mean(dim=1).squeeze(0)\n            else:\n                raise ValueError(\"Cannot extract embedding from model output\")\n            emb_np = emb.cpu().numpy()\n            # Pad or truncate if needed\n            if emb_np.shape[0] < embedding_dim:\n                emb_np = np.pad(emb_np, (0, embedding_dim - emb_np.shape[0]))\n            elif emb_np.shape[0] > embedding_dim:\n                emb_np = emb_np[:embedding_dim]\n            return emb_np\n    except Exception as e:\n        print(f\"ChemBERTa embedding failed for SMILES '{smiles}': {e}\")\n        return np.zeros(embedding_dim)\n    \ndef init_xgb_tuning_db(db_path=\"xgb_tuning.db\"):\n    \"\"\"Initialize the XGB tuning database and return all existing results.\"\"\"\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n    c.execute('''CREATE TABLE IF NOT EXISTS xgb_tuning\n                 (param_hash TEXT PRIMARY KEY, params TEXT, score REAL)''')\n    c.execute('SELECT params, score FROM xgb_tuning')\n    results = c.fetchall()\n    conn.close()\n    return [(json.loads(params), score) for params, score in results]\n\ndef get_param_hash(params):\n    param_str = json.dumps(params, sort_keys=True)\n    return hashlib.md5(param_str.encode('utf-8')).hexdigest()\n\ndef check_db_for_params(db_path, param_hash):\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n    c.execute('SELECT score FROM xgb_tuning WHERE param_hash=?', (param_hash,))\n    result = c.fetchone()\n    conn.close()\n    return result is not None\n\ndef save_result_to_db(db_path, param_hash, params, score):\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n    c.execute('''INSERT OR REPLACE INTO xgb_tuning (param_hash, params, score)\n                 VALUES (?, ?, ?)''', (param_hash, json.dumps(params, sort_keys=True), score))\n    conn.commit()\n    conn.close()\n\n# --- XGB Hyperparameter Grid Search with DB Caching ---\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import ParameterGrid\n\ndef xgb_grid_search_with_db(X, y, param_grid, db_path=\"xgb_tuning.db\"):\n    \"\"\"\n    For each param set in grid, check DB. If not present, train and save result.\n    param_grid: dict of param lists, e.g. {'max_depth':[3,5], 'learning_rate':[0.01,0.1]}\n    \"\"\"\n    tried = 0\n    best_score = None\n    best_params = None\n    # Split X, y into train/val for early stopping\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05, random_state=42)\n    for params in ParameterGrid(param_grid):\n        param_hash = get_param_hash(params)\n        if check_db_for_params(db_path, param_hash):\n            print(f\"Skipping already tried params: {params}\")\n            continue\n        # print(f\"Trying params: {json.dumps(params, sort_keys=True)}\")\n        model = XGBRegressor(**params)\n        # Provide eval_set for early stopping\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n        y_pred = model.predict(X_val)\n        from sklearn.metrics import mean_absolute_error\n        score = mean_absolute_error(y_val, y_pred)\n        print(f\"Result: MAE={score:.6f} for params: {json.dumps(params, sort_keys=True)}\")\n        # For MAE, lower is better\n        if (best_score is None) or (score < best_score):\n            best_score = score\n            best_params = params.copy()\n            print(f\"New best MAE: {best_score:.6f} with params: {json.dumps(best_params, sort_keys=True)}\")\n        save_result_to_db(db_path, param_hash, params, score)\n        tried += 1\n    print(f\"Tried {tried} new parameter sets.\")\n    if best_score is not None:\n        print(f\"Best score overall: {best_score:.6f} with params: {json.dumps(best_params, sort_keys=True)}\")\n\nfrom sklearn.linear_model import RidgeCV, ElasticNetCV\n\ndef drop_correlated_features(df, threshold=0.95):\n    \"\"\"\n    Drops columns in a DataFrame that are highly correlated with other columns.\n    Only one of each pair of correlated columns is kept.\n\n    Parameters:\n        df (pd.DataFrame): Input DataFrame.\n        threshold (float): Correlation threshold for dropping columns (default 0.95).\n\n    Returns:\n        pd.DataFrame: DataFrame with correlated columns dropped.\n        list: List of dropped column names.\n    \"\"\"\n    corr_matrix = df.corr().abs()\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n    return df.drop(columns=to_drop), to_drop\n\ndef get_canonical_smiles(smiles):\n        \"\"\"Convert SMILES to canonical form for consistency\"\"\"\n        if not RDKIT_AVAILABLE:\n            return smiles\n        try:\n            mol = Chem.MolFromSmiles(smiles)\n            if mol:\n                return Chem.MolToSmiles(mol, canonical=True)\n        except:\n            pass\n        return smiles\n\n\"\"\"\nLoad competition data with complete filtering of problematic polymer notation\n\"\"\"\n\nprint(\"Loading competition data...\")\ntrain = pd.read_csv(BASE_PATH + 'train.csv')\ntest = pd.read_csv(BASE_PATH + 'test.csv')\n\nif config.debug:\n    print(\"   Debug mode: sampling 1000 training examples\")\n    train = train.sample(n=1000, random_state=42).reset_index(drop=True)\n\nprint(f\"Training data shape: {train.shape}, Test data shape: {test.shape}\")\n\ndef clean_and_validate_smiles(smiles):\n    \"\"\"Completely clean and validate SMILES, removing all problematic patterns\"\"\"\n    if not isinstance(smiles, str) or len(smiles) == 0:\n        return None\n    \n    # List of all problematic patterns we've seen\n    bad_patterns = [\n        '[R]', '[R1]', '[R2]', '[R3]', '[R4]', '[R5]', \n        \"[R']\", '[R\"]', 'R1', 'R2', 'R3', 'R4', 'R5',\n        # Additional patterns that cause issues\n        '([R])', '([R1])', '([R2])', \n    ]\n    \n    # Check for any bad patterns\n    for pattern in bad_patterns:\n        if pattern in smiles:\n            return None\n    \n    # Additional check: if it contains ] followed by [ without valid atoms, likely polymer notation\n    if '][' in smiles and any(x in smiles for x in ['[R', 'R]']):\n        return None\n    \n    # Try to parse with RDKit if available\n    if RDKIT_AVAILABLE:\n        try:\n            mol = Chem.MolFromSmiles(smiles)\n            if mol is not None:\n                return Chem.MolToSmiles(mol, canonical=True)\n            else:\n                return None\n        except:\n            return None\n    \n    # If RDKit not available, return cleaned SMILES\n    return smiles\n\n# Clean and validate all SMILES\nprint(\"Cleaning and validating SMILES...\")\ntrain['SMILES'] = train['SMILES'].apply(clean_and_validate_smiles)\ntest['SMILES'] = test['SMILES'].apply(clean_and_validate_smiles)\n\n# Remove invalid SMILES\ninvalid_train = train['SMILES'].isnull().sum()\ninvalid_test = test['SMILES'].isnull().sum()\n\nprint(f\"   Removed {invalid_train} invalid SMILES from training data\")\nprint(f\"   Removed {invalid_test} invalid SMILES from test data\")\n\ntrain = train[train['SMILES'].notnull()].reset_index(drop=True)\ntest = test[test['SMILES'].notnull()].reset_index(drop=True)\n\nprint(f\"   Final training samples: {len(train)}\")\nprint(f\"   Final test samples: {len(test)}\")\n\ndef add_extra_data_clean(df_train, df_extra, target):\n    \"\"\"Add external data with thorough SMILES cleaning\"\"\"\n    n_samples_before = len(df_train[df_train[target].notnull()])\n    \n    print(f\"      Processing {len(df_extra)} {target} samples...\")\n    \n    # Clean external SMILES\n    df_extra['SMILES'] = df_extra['SMILES'].apply(clean_and_validate_smiles)\n    \n    # Remove invalid SMILES and missing targets\n    before_filter = len(df_extra)\n    df_extra = df_extra[df_extra['SMILES'].notnull()]\n    df_extra = df_extra.dropna(subset=[target])\n    after_filter = len(df_extra)\n    \n    print(f\"      Kept {after_filter}/{before_filter} valid samples\")\n    \n    if len(df_extra) == 0:\n        print(f\"      No valid data remaining for {target}\")\n        return df_train\n    \n    # Group by canonical SMILES and average duplicates\n    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n    \n    cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n    unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n\n    # Fill missing values\n    filled_count = 0\n    for smile in df_train[df_train[target].isnull()]['SMILES'].tolist():\n        if smile in cross_smiles:\n            df_train.loc[df_train['SMILES']==smile, target] = \\\n                df_extra[df_extra['SMILES']==smile][target].values[0]\n            filled_count += 1\n    \n    # Add unique SMILES\n    extra_to_add = df_extra[df_extra['SMILES'].isin(unique_smiles_extra)].copy()\n    if len(extra_to_add) > 0:\n        for col in TARGETS:\n            if col not in extra_to_add.columns:\n                extra_to_add[col] = np.nan\n        \n        extra_to_add = extra_to_add[['SMILES'] + TARGETS]\n        df_train = pd.concat([df_train, extra_to_add], axis=0, ignore_index=True)\n\n    n_samples_after = len(df_train[df_train[target].notnull()])\n    print(f'      {target}: +{n_samples_after-n_samples_before} samples, +{len(unique_smiles_extra)} unique SMILES')\n    print(f\"      Filled {filled_count} missing entries in train for {target}\")\n    print(f\"      Added {len(extra_to_add)} new entries for {target}\")\n    return df_train\n\n# Load external datasets with robust error handling\nprint(\"\\n📂 Loading external datasets...\")\n\nexternal_datasets = []\n\n# Function to safely load datasets\ndef safe_load_dataset(path, target, processor_func, description):\n    try:\n        if path.endswith('.xlsx'):\n            data = pd.read_excel(path)\n        else:\n            data = pd.read_csv(path)\n        \n        data = processor_func(data)\n        external_datasets.append((target, data))\n        print(f\"   ✅ {description}: {len(data)} samples\")\n        return True\n    except Exception as e:\n        print(f\"   ⚠️ {description} failed: {str(e)[:100]}\")\n        return False\n\n# Load each dataset\nsafe_load_dataset(\n    '/kaggle/input/tc-smiles/Tc_SMILES.csv',\n    'Tc',\n    lambda df: df.rename(columns={'TC_mean': 'Tc'}),\n    'Tc data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv',\n    'Tg', \n    lambda df: df[['SMILES', 'Tg']] if 'Tg' in df.columns else df,\n    'TgSS enriched data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv',\n    'Tg',\n    lambda df: df[['SMILES', 'Tg (C)']].rename(columns={'Tg (C)': 'Tg'}),\n    'JCIM Tg data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/smiles-extra-data/data_tg3.xlsx',\n    'Tg',\n    lambda df: df.rename(columns={'Tg [K]': 'Tg'}).assign(Tg=lambda x: x['Tg'] - 273.15),\n    'Xlsx Tg data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/smiles-extra-data/data_dnst1.xlsx',\n    'Density',\n    lambda df: df.rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]\n                .query('SMILES.notnull() and Density.notnull() and Density != \"nylon\"')\n                .assign(Density=lambda x: x['Density'].astype(float) - 0.118),\n    'Density data'\n)\n\nsafe_load_dataset(\n    BASE_PATH + 'train_supplement/dataset4.csv',\n    'FFV', \n    lambda df: df[['SMILES', 'FFV']] if 'FFV' in df.columns else df,\n    'dataset 4'\n)\n\n# Integrate external data\nprint(\"\\n🔄 Integrating external data...\")\ntrain_extended = train[['SMILES'] + TARGETS].copy()\n\nif getattr(config, \"use_external_data\", True) and  not config.debug:\n    for target, dataset in external_datasets:\n        print(f\"   Processing {target} data...\")\n        train_extended = add_extra_data_clean(train_extended, dataset, target)\n\nprint(f\"\\n📊 Final training data:\")\nprint(f\"   Original samples: {len(train)}\")\nprint(f\"   Extended samples: {len(train_extended)}\")\nprint(f\"   Gain: +{len(train_extended) - len(train)} samples\")\n\nfor target in TARGETS:\n    count = train_extended[target].notna().sum()\n    original_count = train[target].notna().sum() if target in train.columns else 0\n    gain = count - original_count\n    print(f\"   {target}: {count:,} samples (+{gain})\")\n\nprint(f\"\\n✅ Data integration complete with clean SMILES!\")\n\ndef separate_subtables(train_df):\n    labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n    subtables = {}\n    for label in labels:\n        # Filter out NaNs, select columns, reset index\n        subtables[label] = train_df[train_df[label].notna()][['SMILES', label]].reset_index(drop=True)\n\n    # Optional: Debugging\n    for label in subtables:\n        print(f\"{label} NaNs per column:\")\n        print(subtables[label].isna().sum())\n        print(subtables[label].shape)\n        print(\"-\" * 40)\n\n    return subtables\n\ndef augment_smiles_dataset(smiles_list, labels, num_augments=3):\n    \"\"\"\n    Augments a list of SMILES strings by generating randomized versions.\n\n    Parameters:\n        smiles_list (list of str): Original SMILES strings.\n        labels (list or np.array): Corresponding labels.\n        num_augments (int): Number of augmentations per SMILES.\n\n    Returns:\n        tuple: (augmented_smiles, augmented_labels)\n    \"\"\"\n    augmented_smiles = []\n    augmented_labels = []\n\n    for smiles, label in zip(smiles_list, labels):\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None:\n            continue\n        # Add original\n        augmented_smiles.append(smiles)\n        augmented_labels.append(label)\n        # Add randomized versions\n        for _ in range(num_augments):\n            rand_smiles = Chem.MolToSmiles(mol, doRandom=True)\n            augmented_smiles.append(rand_smiles)\n            augmented_labels.append(label)\n\n    return augmented_smiles, np.array(augmented_labels)\n\nmordred_calc = Calculator(mordred_descriptors, ignore_3D=True)\ndef build_mordred_descriptors(smiles_list):\n    # Build Mordred descriptors for test\n    mols_test = [Chem.MolFromSmiles(s) for s in smiles_list]\n    desc_test = mordred_calc.pandas(mols_test, nproc=1)\n\n    # Make columns string & numeric only (no dropping beyond that)\n    desc_test.columns = desc_test.columns.map(str)\n    desc_test = desc_test.select_dtypes(include=[np.number]).copy()\n    desc_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n    return desc_test\n\nfrom rdkit.Chem import Crippen, Lipinski\n\ndef smiles_to_combined_fingerprints_with_descriptors(smiles_list):\n    # Set fingerprint parameters inside the function\n    radius = 2\n    n_bits = 128\n\n    generator = GetMorganGenerator(radius=radius, fpSize=n_bits) if getattr(Config, \"use_morgan_fp\", True) else None\n    atom_pair_gen = GetAtomPairGenerator(fpSize=n_bits) if getattr(Config, 'use_atom_pair_fp', False) else None\n    torsion_gen = GetTopologicalTorsionGenerator(fpSize=n_bits) if getattr(Config, 'use_torsion_fp', False) else None\n    \n    fp_len = (n_bits if getattr(Config, 'use_morgan_fp', False) else 0) \\\n           + (n_bits if getattr(Config, 'use_atom_pair_fp', False) else 0) \\\n           + (n_bits if getattr(Config, 'use_torsion_fp', False) else 0) \\\n           + (167 if getattr(Config, 'use_maccs_fp', True) else 0)\n    if getattr(Config, 'use_chemberta', False):\n        fp_len += 384\n        \n    fingerprints = []\n    descriptors = []\n    valid_smiles = []\n    invalid_indices = []\n    use_any_fp = getattr(Config, \"use_morgan_fp\", False) or getattr(Config, \"use_atom_pair_fp\", False) or getattr(Config, \"use_torsion_fp\", False) or getattr(Config, \"use_maccs_fp\", False) or getattr(Config, 'use_chemberta', False)\n\n    for i, smiles in enumerate(smiles_list):\n        mol = Chem.MolFromSmiles(smiles)\n        if mol:\n            # Fingerprints (No change from your code)\n            if use_any_fp:\n                fps = []\n                if getattr(Config, \"use_morgan_fp\", True) and generator is not None:\n                    fps.append(np.array(generator.GetFingerprint(mol)))\n                if atom_pair_gen:\n                    fps.append(np.array(atom_pair_gen.GetFingerprint(mol)))\n                if torsion_gen:\n                    fps.append(np.array(torsion_gen.GetFingerprint(mol)))\n                if getattr(Config, 'use_maccs_fp', True):\n                    fps.append(np.array(MACCSkeys.GenMACCSKeys(mol)))\n                if getattr(Config, \"use_chemberta\", False):\n                    emb = get_chemberta_embedding(smiles)\n                    fps.append(emb)\n                \n                combined_fp = np.concatenate(fps)\n                fingerprints.append(combined_fp)\n\n            if getattr(Config, 'use_descriptors', True):\n                descriptor_values = {}\n                for name, func in Descriptors.descList:\n                    try:\n                        descriptor_values[name] = func(mol)\n                    except:\n                        print(f\"Descriptor {name} failed for SMILES at index {i}\")\n                        descriptor_values[name] = None\n\n                # try:\n                # --- Features for Rigidity and Complexity (for Tg, FFV) ---\n                try:\n                    num_heavy_atoms = mol.GetNumHeavyAtoms()\n                except Exception as e:\n                    num_heavy_atoms = 0\n\n                # --- Features for Rigidity and Complexity (for Tg, FFV) ---\n                try:\n                    descriptor_values['NumAromaticRings'] = Lipinski.NumAromaticRings(mol)\n                except Exception as e:\n                    descriptor_values['NumAromaticRings'] = None\n                try:\n                    num_sp3_carbons = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 6 and atom.GetHybridization() == Chem.rdchem.HybridizationType.SP3)\n                    descriptor_values['FractionCSP3'] = num_sp3_carbons / num_heavy_atoms if num_heavy_atoms > 0 else 0\n                except Exception as e:\n                    descriptor_values['FractionCSP3'] = None\n\n                # --- Features for Bulkiness and Shape (for FFV) ---\n                try:\n                    descriptor_values['MolMR'] = Crippen.MolMR(mol) # Molar Refractivity (volume)\n                except Exception as e:\n                    descriptor_values['MolMR'] = None\n                try:\n                    descriptor_values['LabuteASA'] = Descriptors.LabuteASA(mol) # Accessible surface area\n                except Exception as e:\n                    descriptor_values['LabuteASA'] = None\n\n                # --- Features for Heavy Atoms (for Density) ---\n                try:\n                    descriptor_values['NumFluorine'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 9)\n                except Exception as e:\n                    descriptor_values['NumFluorine'] = None\n                try:\n                    descriptor_values['NumChlorine'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 17)\n                except Exception as e:\n                    descriptor_values['NumChlorine'] = None\n\n                # --- Features for Intermolecular Forces (for Tc) ---\n                try:\n                    descriptor_values['NumHDonors'] = Lipinski.NumHDonors(mol)\n                except Exception as e:\n                    descriptor_values['NumHDonors'] = None\n                try:\n                    descriptor_values['NumHAcceptors'] = Lipinski.NumHAcceptors(mol)\n                except Exception as e:\n                    descriptor_values['NumHAcceptors'] = None\n\n                # --- Features for Branching and Flexibility (for Rg) ---\n                try:\n                    descriptor_values['BalabanJ'] = Descriptors.BalabanJ(mol) # Topological index sensitive to branching\n                except Exception as e:\n                    descriptor_values['BalabanJ'] = None\n                try:\n                    descriptor_values['Kappa2'] = Descriptors.Kappa2(mol) # Molecular shape index\n                except Exception as e:\n                    descriptor_values['Kappa2'] = None\n                try:\n                    descriptor_values['NumRotatableBonds'] = CalcNumRotatableBonds(mol) # Flexibility\n                except Exception as e:\n                    descriptor_values['NumRotatableBonds'] = None\n                \n                # Graph-based features\n                try:\n                    adj = rdmolops.GetAdjacencyMatrix(mol)\n                    G = nx.from_numpy_array(adj)\n                    if nx.is_connected(G):\n                        descriptor_values['graph_diameter'] = nx.diameter(G)\n                        descriptor_values['avg_shortest_path'] = nx.average_shortest_path_length(G)\n                    else:\n                        descriptor_values['graph_diameter'], descriptor_values['avg_shortest_path'] = 0, 0\n                    descriptor_values['num_cycles'] = len(list(nx.cycle_basis(G)))\n                except:\n                    print(f\"Graph features failed for SMILES at index {i}\")\n                    descriptor_values['graph_diameter'], descriptor_values['avg_shortest_path'], descriptor_values['num_cycles'] = None, None, None\n\n                descriptors.append(descriptor_values)\n            else:\n                descriptors.append(None)\n            valid_smiles.append(smiles)\n        else:\n            if use_any_fp: fingerprints.append(np.zeros(fp_len))\n            if getattr(Config, \"use_chemberta\", False):\n                descriptors.append({f'chemberta_emb_{j}': 0.0 for j in range(384)})\n            else:\n                descriptors.append(None)\n            valid_smiles.append(None)\n            invalid_indices.append(i)\n\n    fingerprints_df = pd.DataFrame(fingerprints, columns=[f'FP_{i}' for i in range(fp_len)]) if use_any_fp else pd.DataFrame()\n    descriptors_df = pd.DataFrame([d for d in descriptors if d is not None]) if any(d is not None for d in descriptors) else pd.DataFrame()\n\n    if getattr(Config, 'use_mordred', False):\n        mordred_df = build_mordred_descriptors(smiles_list)\n        if descriptors_df.empty:\n            descriptors_df = mordred_df\n        else:\n            descriptors_df = pd.concat([descriptors_df.reset_index(drop=True), mordred_df], axis=1)\n    \n    # Keep only unique columns in descriptors_df\n    if not descriptors_df.empty:\n        descriptors_df = descriptors_df.loc[:, ~descriptors_df.columns.duplicated()]\n    return fingerprints_df, descriptors_df, valid_smiles, invalid_indices\n\nrequired_descriptors = {'graph_diameter','num_cycles','avg_shortest_path','MolWt', 'LogP', 'TPSA', 'RotatableBonds', 'NumAtoms'}\n\n# Utility function to combine train and val sets into X_all and y_all\ndef combine_train_val(X_train, X_val, y_train, y_val):\n    X_train = pd.DataFrame(X_train) if isinstance(X_train, np.ndarray) else X_train\n    X_val = pd.DataFrame(X_val) if isinstance(X_val, np.ndarray) else X_val\n    y_train = pd.Series(y_train) if isinstance(y_train, np.ndarray) else y_train\n    y_val = pd.Series(y_val) if isinstance(y_val, np.ndarray) else y_val\n    X_all = pd.concat([X_train, X_val], axis=0)\n    y_all = pd.concat([y_train, y_val], axis=0)\n    return X_all, y_all\n\n# --- PCA utility for train/test transformation ---\ndef apply_pca(X_train, X_test=None, verbose=True):\n    pca = PCA(n_components=config.pca_variance, svd_solver='full', random_state=getattr(config, 'random_state', 42))\n    X_train_pca = pca.fit_transform(X_train)\n    X_test_pca = pca.transform(X_test) if X_test is not None else None\n    if verbose:\n        print(f\"[PCA] Reduced train shape: {X_train.shape} -> {X_train_pca.shape} (kept {pca.n_components_} components, {100*pca.explained_variance_ratio_.sum():.4f}% variance)\")\n    return X_train_pca, X_test_pca, pca\n\ndef augment_dataset(X, y, n_samples=1000, n_components=5, random_state=None):\n    \"\"\"\n    Augments a dataset using Gaussian Mixture Models.\n\n    Parameters:\n    - X: pd.DataFrame or np.ndarray — feature matrix\n    - y: pd.Series or np.ndarray — target values\n    - n_samples: int — number of synthetic samples to generate\n    - n_components: int — number of GMM components\n    - random_state: int — random seed for reproducibility\n\n    Returns:\n    - X_augmented: pd.DataFrame — augmented feature matrix\n    - y_augmented: pd.Series — augmented target values\n    \"\"\"\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n    elif not isinstance(X, pd.DataFrame):\n        raise ValueError(\"X must be a pandas DataFrame or a NumPy array\")\n\n    X.columns = X.columns.astype(str)\n\n    if isinstance(y, np.ndarray):\n        y = pd.Series(y)\n    elif not isinstance(y, pd.Series):\n        raise ValueError(\"y must be a pandas Series or a NumPy array\")\n\n    df = X.copy()\n    df['Target'] = y.values\n\n    gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n    gmm.fit(df)\n\n    synthetic_data, _ = gmm.sample(n_samples)\n    synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n\n    augmented_df = pd.concat([df, synthetic_df], ignore_index=True)\n\n    X_augmented = augmented_df.drop(columns='Target')\n    y_augmented = augmented_df['Target']\n\n    return X_augmented, y_augmented\n\n# --- Outlier Detection Summary Function ---\ndef display_outlier_summary(y, X=None, name=\"target\", z_thresh=3, iqr_factor=1.5, iso_contamination=0.01, lof_contamination=0.01):\n    \"\"\"\n    Display the percentage of data flagged as outlier by Z-score, IQR, Isolation Forest, and LOF.\n    y: 1D array-like (target or feature)\n    X: 2D array-like (feature matrix, required for Isolation Forest/LOF)\n    name: str, name of the variable being checked\n    \"\"\"\n    print(f\"\\nOutlier summary for: {name}\")\n    y = np.asarray(y)\n    n = len(y)\n    # Z-score\n    z_scores = (y - np.mean(y)) / np.std(y)\n    z_outliers = np.abs(z_scores) > z_thresh\n    print(f\"Z-score > {z_thresh}: {np.sum(z_outliers)} / {n} ({100*np.mean(z_outliers):.2f}%)\")\n\n    # IQR\n    Q1 = np.percentile(y, 25)\n    Q3 = np.percentile(y, 75)\n    IQR = Q3 - Q1\n    lower = Q1 - iqr_factor * IQR\n    upper = Q3 + iqr_factor * IQR\n    iqr_outliers = (y < lower) | (y > upper)\n    print(f\"IQR (factor {iqr_factor}): {np.sum(iqr_outliers)} / {n} ({100*np.mean(iqr_outliers):.2f}%)\")\n\n    # Isolation Forest (if X provided)\n    if X is not None:\n        try:\n            from sklearn.ensemble import IsolationForest\n            iso = IsolationForest(contamination=iso_contamination, random_state=42)\n            iso_out = iso.fit_predict(X)\n            iso_outliers = iso_out == -1\n            print(f\"Isolation Forest (contamination={iso_contamination}): {np.sum(iso_outliers)} / {len(iso_outliers)} ({100*np.mean(iso_outliers):.2f}%)\")\n        except Exception as e:\n            print(f\"Isolation Forest failed: {e}\")\n        # Local Outlier Factor\n        try:\n            from sklearn.neighbors import LocalOutlierFactor\n            lof = LocalOutlierFactor(n_neighbors=20, contamination=lof_contamination)\n            lof_out = lof.fit_predict(X)\n            lof_outliers = lof_out == -1\n            print(f\"Local Outlier Factor (contamination={lof_contamination}): {np.sum(lof_outliers)} / {len(lof_outliers)} ({100*np.mean(lof_outliers):.2f}%)\")\n        except Exception as e:\n            print(f\"Local Outlier Factor failed: {e}\")\n    else:\n        print(\"Isolation Forest/LOF skipped (X not provided)\")\n\n\ntrain_df=train_extended\ntest_df=test\nsubtables = separate_subtables(train_df)\n\ntest_smiles = test_df['SMILES'].tolist()\ntest_ids = test_df['id'].values\nlabels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n#labels = ['Tc']\n\n# Save importance_df to Excel log file, one sheet per label\ndef save_importance_to_excel(importance_df, label, log_path):\n    import os\n    from openpyxl import load_workbook\n    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n    if os.path.exists(log_path):\n        with pd.ExcelWriter(log_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n            importance_df.to_excel(writer, sheet_name=label, index=False)\n    else:\n        with pd.ExcelWriter(log_path, engine='openpyxl') as writer:\n            importance_df.to_excel(writer, sheet_name=label, index=False)\n\ndef get_least_important_features_all_methods(X, y, label, model_name=None):\n    \"\"\"\n    Remove features in three steps:\n    1. Remove features with model.feature_importances_ <= 0\n    2. Remove features with permutation_importance <= 0\n    3. Remove features with SHAP importance <= 0\n    Returns a list of features to remove (union of all three criteria).\n    \"\"\"    \n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.05, random_state=config.random_state)\n    model_type = (model_name or getattr(config, 'model_name', 'xgb'))\n    if model_type == 'xgb':\n        model = XGBRegressor(random_state=config.random_state, n_jobs=-1, verbosity=0, early_stopping_rounds=50, eval_metric=\"mae\", objective=\"reg:absoluteerror\")\n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n    elif model_type == 'catboost':\n        model = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6, loss_function='MAE', eval_metric='MAE', random_seed=config.random_state, verbose=False)\n        model.fit(X_train, y_train, eval_set=(X_valid, y_valid), early_stopping_rounds=50, use_best_model=True)\n    elif model_type == 'lgbm':\n        model = LGBMRegressor(n_estimators=1000, learning_rate=0.05, max_depth=6, reg_lambda=1.0, objective='mae', random_state=config.random_state, verbose=-1, verbosity=-1)\n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric='mae', callbacks=[lgb.early_stopping(stopping_rounds=50)])\n    else:\n        model = XGBRegressor(random_state=config.random_state, n_jobs=-1, verbosity=0, early_stopping_rounds=50, eval_metric=\"rmse\")\n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n    feature_names = X_train.columns\n\n    # 1. Remove features with model.feature_importances_ <= 0\n    fi_mask = model.feature_importances_ <= 0\n    fi_features = set(feature_names[fi_mask])\n    # Save feature_importances_ to Excel, sorted by importance\n    fi_importance_df = pd.DataFrame({\n        'feature': feature_names,\n        'importance_mean': model.feature_importances_,\n        'importance_std': [0]*len(feature_names)\n    }).sort_values('importance_mean', ascending=False)\n    save_importance_to_excel(fi_importance_df, label + '_fi', getattr(Config, 'permutation_importance_log_path', 'log/permutation_importance_log.xlsx'))\n\n    # 2. Remove features with permutation_importance <= 0\n    perm_result = permutation_importance(\n        model, X_valid, y_valid,\n        n_repeats=1 if config.debug else 10,\n        random_state=config.random_state,\n        scoring='neg_mean_absolute_error'\n    )\n    # Save permutation importance to Excel, sorted by mean importance descending\n    perm_importance_df = pd.DataFrame({\n        'feature': feature_names,\n        'importance_mean': perm_result.importances_mean,\n        'importance_std': perm_result.importances_std\n    }).sort_values('importance_mean', ascending=False)\n    save_importance_to_excel(perm_importance_df, label + '_perm', getattr(Config, 'permutation_importance_log_path', 'log/permutation_importance_log.xlsx'))\n    perm_mask = perm_result.importances_mean <= 0\n    perm_features = set(feature_names[perm_mask])\n\n    # 3. Remove features with SHAP importance <= 0\n    explainer = shap.Explainer(model, X_valid)\n    # For LGBM, disable additivity check to avoid ExplainerError\n    if model_type == 'lgbm':\n        shap_values = explainer(X_valid, check_additivity=False)\n    else:\n        shap_values = explainer(X_valid)\n    shap_importance = np.abs(shap_values.values).mean(axis=0)\n    shap_mask = shap_importance <= 0\n    shap_features = set(feature_names[shap_mask])\n    # Save SHAP importance to Excel, sorted by importance\n    shap_importance_df = pd.DataFrame({\n        'feature': feature_names,\n        'importance_mean': shap_importance,\n        'importance_std': [0]*len(feature_names)\n    }).sort_values('importance_mean', ascending=False)\n    save_importance_to_excel(shap_importance_df, label + '_shap', getattr(Config, 'permutation_importance_log_path', 'log/permutation_importance_log.xlsx'))\n\n    # Union of all features to remove\n    features_to_remove = fi_features | perm_features | shap_features\n    print(f\"Removed {len(features_to_remove)} features for {label} using all methods (fi: {len(fi_features)}, perm: {len(perm_features)}, shap: {len(shap_features)})\")\n\n    return list(features_to_remove)\n\ndef get_least_important_features(X, y, label, model_name=None):\n    # Correct unpacking of train_test_split\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.05, random_state=config.random_state)\n\n    if (model_name or getattr(config, 'model_name', 'xgb')) == 'xgb':\n        model = XGBRegressor(random_state=config.random_state, n_jobs=-1, verbosity=0, early_stopping_rounds=50, eval_metric=\"mae\", objective=\"reg:absoluteerror\")\n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n    else:\n        model = ExtraTreesRegressor(random_state=config.random_state, criterion='absolute_error')\n        model.fit(X, y)\n\n    # Use config.feature_importance_method to choose method\n    importance_method = getattr(config, 'feature_importance_method', 'feature_importances_')\n    if importance_method == 'feature_importances_':\n        importance_df = pd.DataFrame({\n            'feature': X_train.columns,\n            'importance_mean': model.feature_importances_,\n            'importance_std': [0]*len(X_train.columns)\n        })\n    else:\n    # model = ExtraTreesRegressor(random_state=config.random_state)\n        result = permutation_importance(\n            model, X_valid, y_valid,\n            n_repeats=30,\n            random_state=Config.random_state,\n            scoring='r2'\n        )\n        importance_df = pd.DataFrame({\n            'feature': X_train.columns,\n            'importance_mean': result.importances_mean,\n            'importance_std': result.importances_std\n        })\n\n    # Use model- and label-specific n_least_important_features\n    if model_name is None:\n        model_name_used = getattr(config, 'model_name', 'xgb')\n    else:\n        model_name_used = model_name\n    n = config.n_least_important_features.get(model_name_used, {}).get(label, 5)\n    # Sort by importance (ascending) and return n least important features\n    # Remove all features with importance_mean < 0 first\n    negative_importance = importance_df[importance_df['importance_mean'] <= 0]\n    num_negative = len(negative_importance)\n    least_important = negative_importance\n\n    # If less than n features removed, remove more to reach n\n    if num_negative < n:\n        # Exclude already selected features\n        remaining = importance_df[~importance_df['feature'].isin(negative_importance['feature'])]\n        additional = remaining.sort_values(by='importance_mean').head(n - num_negative)\n        least_important = pd.concat([least_important, additional], ignore_index=True)\n    else:\n        # If already removed n or more, just keep the negative ones\n        least_important = negative_importance\n\n    print(f\"Removed {len(least_important)} least important features for {label} (with {num_negative} <= 0)\")\n\n    importance_df = importance_df.sort_values(by='importance_mean', ascending=True)\n\n    # Mark features to be removed\n    importance_df['removed'] = importance_df['feature'].isin(least_important['feature'])\n\n    save_importance_to_excel(importance_df, label, Config.permutation_importance_log_path)\n\n    return least_important['feature'].tolist()\n\n# Save model to disk for this fold using a helper function\ndef save_model(Model, label, fold, model_name):\n    model_path = f\"models/{label}_fold{fold+1}_{model_name}\"\n    try:\n        if 'torch' in str(type(Model)).lower():\n            # Save PyTorch model state_dict\n            model_path += \".pt\"\n            torch.save(Model.state_dict(), model_path)\n        else:\n            # Save scikit-learn model\n            model_path += \".joblib\"\n            joblib.dump(Model, model_path)\n        print(f\"Saved model for {label} fold {fold+1} to {model_path}\")\n    except Exception as e:\n        print(f\"Failed to save model for {label} fold {fold+1}: {e}\")\n\ndef train_with_other_models(model_name, label, X_train, y_train, X_val, y_val):\n    \"\"\"\n    Train a regression model using the specified model_name, with hyperparameters\n    adapted to the data size of the target label.\n    \"\"\"\n    print(f\"Training {model_name} model for label: {label}\")\n    if model_name == 'tabnet':\n        try:\n            from pytorch_tabnet.tab_model import TabNetRegressor\n        except ImportError:\n            raise ImportError(\"pytorch-tabnet is not installed. Please install it with 'pip install pytorch-tabnet'.\")\n        \n        # --- Define TabNet parameters based on label ---\n        if label in ['Rg', 'Tc']: # Low Data\n            params = {'n_d': 8, 'n_a': 8, 'n_steps': 3, 'gamma': 1.3, 'lambda_sparse': 1e-4}\n        elif label == 'FFV': # High Data\n            params = {'n_d': 24, 'n_a': 24, 'n_steps': 5, 'gamma': 1.5, 'lambda_sparse': 1e-5}\n        else: # Medium Data\n            params = {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5, 'lambda_sparse': 1e-5}\n\n        Model = TabNetRegressor(**params, seed=42, verbose=0)\n        Model.fit(\n            X_train.values, y_train.values.reshape(-1, 1),\n            eval_set=[(X_val.values, y_val.values.reshape(-1, 1))],\n            eval_metric=['mae'], # ACTION: Changed from 'rmse' to 'mae'\n            max_epochs=200, patience=20, batch_size=1024, virtual_batch_size=128\n        )\n\n    elif model_name == 'catboost':\n        # --- Define CatBoost parameters based on label ---\n        params = {'iterations': 3000, 'learning_rate': 0.05, 'loss_function': 'MAE', 'eval_metric': 'MAE', 'random_seed': Config.random_state, 'verbose': False}\n        if label in ['Rg', 'Tc']: # Low Data\n            params.update({'depth': 5, 'l2_leaf_reg': 7})\n        elif label == 'FFV': # High Data\n            params.update({'depth': 7, 'l2_leaf_reg': 2})\n        else: # Medium Data\n            params.update({'depth': 6, 'l2_leaf_reg': 3})\n\n        Model = CatBoostRegressor(**params)\n        Model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, use_best_model=True)\n\n    elif model_name == 'lgbm':\n        # --- Define LightGBM parameters based on label ---\n        params = {'n_estimators': 3000, 'learning_rate': 0.05, 'objective': 'mae', 'random_state': Config.random_state, 'verbose': -1, 'verbosity': -1}\n        if label in ['Rg', 'Tc']: # Low Data\n            params.update({'max_depth': 4, 'num_leaves': 20, 'reg_lambda': 5.0})\n        elif label == 'FFV': # High Data\n            params.update({'max_depth': 7, 'num_leaves': 40, 'reg_lambda': 1.0})\n        else: # Medium Data\n            params.update({'max_depth': 6, 'num_leaves': 31, 'reg_lambda': 1.0})\n\n        Model = LGBMRegressor(**params)\n        Model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='mae', callbacks=[lgb.early_stopping(stopping_rounds=50)])\n\n    elif model_name == 'extratrees':\n        # --- Define ExtraTrees parameters based on label ---\n        params = {'n_estimators': 300, 'criterion': 'absolute_error', 'random_state': Config.random_state, 'n_jobs': -1}\n        if label in ['Rg', 'Tc']: # Low Data: Prevent overfitting by requiring more samples per leaf\n            params.update({'min_samples_leaf': 3, 'max_features': 0.8})\n        else: # High/Medium Data\n            params.update({'min_samples_leaf': 1, 'max_features': 1.0})\n            \n        Model = ExtraTreesRegressor(**params)\n        X_all, y_all = combine_train_val(X_train, X_val, y_train, y_val)\n        Model.fit(X_all, y_all)\n\n    elif model_name == 'randomforest':\n        # --- Define RandomForest parameters based on label ---\n        params = {'n_estimators': 1000, 'criterion': 'absolute_error', 'random_state': Config.random_state, 'n_jobs': -1}\n        if label in ['Rg', 'Tc']: # Low Data\n            params.update({'min_samples_leaf': 3, 'max_features': 0.8, 'max_depth': 15})\n        else: # High/Medium Data\n            params.update({'min_samples_leaf': 1, 'max_features': 1.0, 'max_depth': None})\n\n        Model = RandomForestRegressor(**params)\n        X_all, y_all = combine_train_val(X_train, X_val, y_train, y_val)\n        Model.fit(X_all, y_all)\n\n    elif model_name == 'hgbm':\n        from sklearn.ensemble import HistGradientBoostingRegressor\n        # --- Define HGBM parameters based on label ---\n        params = {'max_iter': 1000, 'learning_rate': 0.05, 'loss': 'absolute_error', 'early_stopping': True, 'random_state': 42} # ACTION: Changed loss to 'absolute_error'\n        if label in ['Rg', 'Tc']: # Low Data\n            params.update({'max_depth': 4, 'l2_regularization': 1.0})\n        elif label == 'FFV': # High Data\n            params.update({'max_depth': 7, 'l2_regularization': 0.1})\n        else: # Medium Data\n            params.update({'max_depth': 6, 'l2_regularization': 0.5})\n\n        Model = HistGradientBoostingRegressor(**params)\n        X_all, y_all = combine_train_val(X_train, X_val, y_train, y_val)\n        Model.fit(X_all, y_all)\n\n    elif model_name == 'nn':\n        # The 'train_with_nn' function already uses different configs per label, which is excellent!\n        # Just ensure the loss function inside it is nn.L1Loss()\n        Model = train_with_nn(label, X_train, X_val, y_train, y_val)\n    else:\n        raise ValueError(f\"Unknown or unavailable model: {model_name}\")\n    \n    return Model\n\ndef train_with_autogluon(label, X_train, y_train):\n    try:\n        from autogluon.tabular import TabularPredictor\n    except ImportError:\n        raise ImportError(\"AutoGluon is not installed. Please install it with 'pip install autogluon'.\")\n    import pandas as pd\n    import uuid\n    # Prepare data for AutoGluon (must be DataFrame with column names)\n    X_train_df = pd.DataFrame(X_train)\n    y_train_series = pd.Series(y_train, name=label)\n    train_data = X_train_df.copy()\n    train_data[label] = y_train_series.values\n\n    unique_path = f\"autogluon_{label}_{int(time.time())}_{uuid.uuid4().hex}\"\n\n    hyperparameters = {\n        \"GBM\": {},\n        \"CAT\": {},\n        \"XGB\": {},\n        \"NN_TORCH\": {},\n        \"RF\": {},\n        \"XT\": {}\n    }\n\n    hyperparameter_tune_kwargs = {\n        \"num_trials\": 50,\n        \"scheduler\": \"local\",\n        \"searcher\": \"auto\"\n    }\n\n    time_limit = 300 if getattr(Config, 'debug', False) else 3600\n\n    predictor = TabularPredictor(\n        label=label,\n        eval_metric=\"mae\",  # Use 'mae' for regression\n        path=unique_path\n    ).fit(\n        train_data,\n        presets=\"best_quality\",\n        hyperparameters=hyperparameters,\n        hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n        num_bag_folds=5,\n        num_stack_levels=2,\n        time_limit=time_limit\n    )\n\n    print(\"\\n[AutoGluon] Leaderboard:\")\n    leaderboard = predictor.leaderboard(silent=False)\n\n    print(\"\\n[AutoGluon] Model Info:\")\n    print(predictor.info())\n\n    print(\"\\n[AutoGluon] Model Names:\")\n    model_names = leaderboard[\"model\"].tolist()   # <- FIXED here\n    print(model_names)\n\n    # Save feature importance to CSV\n    fi_df = predictor.feature_importance(train_data)\n    fi_path = f\"NeurIPS/autogluon_feature_importance_{label}.csv\"  \n    fi_df.to_csv(fi_path)\n    print(f\"[AutoGluon] Feature importance saved to {fi_path}\")\n\n    return predictor\n\ndef train_with_stacking(label, X_train, y_train):\n    \"\"\"\n    Trains XGBoost, ExtraTrees, and CatBoost using sklearn's StackingRegressor.\n    Returns the fitted stacking model and base models.\n    \"\"\"\n    estimators = [\n    ('xgb', XGBRegressor(n_estimators=10000, learning_rate=0.01, max_depth=5, subsample=0.8, colsample_bytree=0.8, gamma=0.1, reg_lambda=1.0, objective=\"reg:absoluteerror\", random_state=Config.random_state, n_jobs=-1)),\n    ('lgbm', LGBMRegressor(n_estimators=10000, learning_rate=0.01, num_leaves=64, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=5, reg_lambda=1.0, max_depth=-1, objective=\"mae\", random_state=Config.random_state, n_jobs=-1, verbose=-1)),\n        ('cb', CatBoostRegressor(iterations=10000, learning_rate=0.01, depth=7, l2_leaf_reg=5, bagging_temperature=0.8, random_seed=Config.random_state, verbose=0)),\n    ('rf', RandomForestRegressor(n_estimators=500, max_depth=12, min_samples_leaf=3, random_state=Config.random_state, n_jobs=-1, criterion='absolute_error'))\n    ]\n\n    # Candidate final estimators\n    final_estimators = {\n        \"ElasticNet\": ElasticNetCV(l1_ratio=[0.1, 0.5, 0.9], n_alphas=100, max_iter=50000, tol=1e-3, cv=5, n_jobs=-1),\n        \"CatBoost\": CatBoostRegressor(iterations=3000, learning_rate=0.03, depth=6, l2_leaf_reg=3, random_seed=Config.random_state, verbose=0),\n    \"LightGBM\": LGBMRegressor(n_estimators=3000, learning_rate=0.03, num_leaves=64, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=5, reg_lambda=1.0, max_depth=-1, objective=\"mae\", random_state=Config.random_state, n_jobs=-1, verbose=-1),\n    \"XGBoost\": XGBRegressor(n_estimators=3000, learning_rate=0.03, max_depth=6, subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, gamma=0.1, objective=\"reg:absoluteerror\", random_state=Config.random_state, n_jobs=-1)\n    }\n\n    final_estimator = final_estimators['ElasticNet']\n    \n    stacker = StackingRegressor(estimators=estimators, final_estimator=final_estimator, passthrough=True, cv=5, n_jobs=-1)\n\n    stacker.fit(X_train, y_train)\n    return stacker\n\ndef train_with_xgb(label, X_train, y_train, X_val, y_val):\n    print(f\"Training XGB model for label: {label}\")\n    if label==\"Tg\": # Medium Data (~1.2k samples)\n        Model = XGBRegressor(\n            n_estimators=10000, learning_rate=0.01, max_depth=5, # <-- Reduced from 6\n            colsample_bytree=1.0, reg_lambda=7.0, gamma=0.1, subsample=0.5, # <-- Increased lambda, reduced subsample\n            objective=\"reg:absoluteerror\", random_state=Config.random_state, \n            early_stopping_rounds=50, eval_metric=\"mae\"\n        )\n    elif label=='Rg': # Very Low Data (~600 samples), HIGHEST PRIORITY\n        Model = XGBRegressor(\n            n_estimators=10000, learning_rate=0.06, max_depth=4, \n            colsample_bytree=1.0, reg_lambda=10.0, gamma=0.1, subsample=0.6, \n            objective=\"reg:absoluteerror\", random_state=Config.random_state, \n            early_stopping_rounds=50, eval_metric=\"mae\"\n        )\n    elif label=='FFV': # High Data (~8k samples), LOWEST PRIORITY\n        Model = XGBRegressor(\n            n_estimators=10000, learning_rate=0.06, max_depth=7, \n            colsample_bytree=0.8, reg_lambda=2.0, gamma=0.0, subsample=0.6, \n            objective=\"reg:absoluteerror\", random_state=Config.random_state, \n            early_stopping_rounds=50, eval_metric=\"mae\"\n        )\n    elif label=='Tc': # Low Data (~900 samples), HIGH PRIORITY\n        Model = XGBRegressor(\n            n_estimators=10000, learning_rate=0.01, max_depth=4, \n            colsample_bytree=0.8, reg_lambda=7.0, gamma=0.0, subsample=0.6, \n            objective=\"reg:absoluteerror\", random_state=Config.random_state, \n            early_stopping_rounds=50, eval_metric=\"mae\"\n        )\n    elif label=='Density': # Medium Data (~1.2k samples)\n        Model = XGBRegressor(\n            n_estimators=10000, learning_rate=0.06, max_depth=5, \n            colsample_bytree=1.0, reg_lambda=3.0, gamma=0.0, subsample=0.8, \n            objective=\"reg:absoluteerror\", random_state=Config.random_state, \n            early_stopping_rounds=50, eval_metric=\"mae\"\n        )\n        \n    print(f\"Model {label} trained with shape: {X_train.shape}, {y_train.shape}\")\n\n    Model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        verbose=False,\n    )\n\n    return Model\n\ndef preprocess_numerical_features(X, label=None):\n    # Ensure numeric types\n    X_num = X.select_dtypes(include=[np.number]).copy()\n    \n    # Replace inf/-inf with NaN\n    X_num.replace([np.inf, -np.inf], np.nan, inplace=True)\n    \n    valid_cols = X_num.columns\n    median_values = X_num.median()\n    # Scale features if enabled\n    if getattr(Config, 'use_standard_scaler', False):\n        scaler = StandardScaler()\n        X_num_scaled = scaler.fit_transform(X_num)\n        X_num = pd.DataFrame(X_num_scaled, columns=valid_cols, index=X.index)\n    else:\n        scaler = None\n        X_num = X_num.copy()\n    \n    # Display categorical features\n    cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n    if cat_cols:\n        print(f\"Categorical (non-numeric) features for {label}: {cat_cols}\")\n    else:\n        print(f\"No categorical (non-numeric) features for {label}.\")\n    return X_num, valid_cols, scaler, median_values\n\ndef select_features_with_lasso(X, y, label):\n    X_filled = X.fillna(X.median())\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_filled)\n\n    lasso_cv = LassoCV(cv=3, random_state=432, n_jobs=-1, max_iter=2000)\n\n    feature_selector = SelectFromModel(lasso_cv, prefit=False, threshold=None)\n\n    print(f\"[{label}] Fitting LassoCV to find optimal features...\")\n    feature_selector.fit(X_scaled, y)\n\n    # Get the names of the features that were kept\n    selected_feature_names = X.columns[feature_selector.get_support()]\n\n    print(f\"[{label}] Original number of features: {X.shape[1]}\")\n    print(f\"[{label}] Features selected by Lasso: {len(selected_feature_names)}\")\n\n    # Return the original DataFrame with only the selected columns\n    return selected_feature_names\n\n\ndef check_inf_nan(X, y, label=None):\n    \"\"\"\n    Checks for inf, -inf, and NaN values in X (DataFrame) and y (array/Series).\n    Prints summary and returns True if any such values are found.\n    \"\"\"\n    X_inf = np.isinf(X.values).any()\n    X_nan = np.isnan(X.values).any()\n    y_inf = np.isinf(y).any()\n    y_nan = np.isnan(y).any()\n    if label is None:\n        label = \"\"\n    else:\n        label = f\" [{label}]\"\n    if X_inf or X_nan or y_inf or y_nan:\n        print(f\"⚠️ Detected inf/nan in X or y{label}: X_inf={X_inf}, X_nan={X_nan}, y_inf={y_inf}, y_nan={y_nan}\")\n        if X_inf:\n            print(f\"  X columns with inf: {X.columns[np.isinf(X.values).any(axis=0)].tolist()}\")\n        if X_nan:\n            print(f\"  X columns with nan: {X.columns[np.isnan(X.values).any(axis=0)].tolist()}\")\n        if y_inf:\n            print(\"  y contains inf values.\")\n        if y_nan:\n            print(\"  y contains nan values.\")\n        return True\n    else:\n        print(f\"No inf/nan in X or y{label}.\")\n        return False\n\n# Utility: Display model summary if torchinfo is available\ndef show_model_summary(model, input_dim, batch_size=32):\n    try:\n        from torchinfo import summary\n        print(summary(model, input_size=(batch_size, input_dim)))\n    except ImportError:\n        print(\"torchinfo is not installed. Install it with 'pip install torchinfo' to see model summaries.\")\n\n\ndef train_model(\n    model,\n    X_train, X_val, y_train, y_val,\n    epochs=3000, batch_size=32, lr=1e-3, weight_decay=1e-4, patience=30, verbose=True\n):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n\n    X_train = np.asarray(X_train)\n    y_train = np.asarray(y_train)\n    X_val = np.asarray(X_val)\n    y_val = np.asarray(y_val)\n\n    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n\n    criterion = nn.L1Loss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=verbose)\n\n    best_val_loss = float('inf')\n    best_model_state = None\n    epochs_no_improve = 0\n    use_early_stopping = X_val is not None and y_val is not None\n\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_loader:\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n        if verbose and (epoch+1) % 100 == 0:\n            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n\n        if use_early_stopping:\n            model.eval()\n            with torch.no_grad():\n                val_preds = model(X_val_tensor)\n                val_loss = criterion(val_preds, y_val_tensor).item()\n            scheduler.step(val_loss)\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                best_model_state = model.state_dict()\n                epochs_no_improve = 0\n            else:\n                epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                if verbose:\n                    print(f\"Early stopping at epoch {epoch+1}, best val loss: {best_val_loss:.4f}\")\n                if best_model_state is not None:\n                    model.load_state_dict(best_model_state)\n                break\n\n    return model\n\nclass FeedforwardNet(nn.Module):\n    def __init__(self, input_dim, neurons, dropouts):\n        super().__init__()\n        layers = []\n        for i, n in enumerate(neurons):\n            layers.append(nn.Linear(input_dim, n))\n            # layers.append(nn.BatchNorm1d(n))\n            layers.append(nn.ReLU())\n            if i < len(dropouts) and dropouts[i] > 0:\n                layers.append(nn.Dropout(dropouts[i]))\n            input_dim = n\n        layers.append(nn.Linear(input_dim, 1))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\ndef train_with_nn(label, X_train, X_val, y_train, y_val):\n\n    input_dim = X_train.shape[1]\n    \n    if getattr(Config, \"search_nn\", False):\n        print(f\"--- Starting targeted NN architecture search for label: {label} ---\")\n        \n        search_configs_by_label = {\n            'Low': [ # For Rg, Tc. Simple models with high regularization.\n                # --- Single Layer Focus ---\n                {\"neurons\": [32], \"dropouts\": [0.3]},\n                {\"neurons\": [64], \"dropouts\": [0.4]},\n                {\"neurons\": [128], \"dropouts\": [0.5]},\n                {\"neurons\": [256], \"dropouts\": [0.5]},\n\n                # --- Two Layer Rectangular Focus (based on Rg's winner) ---\n                {\"neurons\": [64, 64], \"dropouts\": [0.4, 0.4]},\n                {\"neurons\": [96, 96], \"dropouts\": [0.5, 0.5]},\n                {\"neurons\": [128, 128], \"dropouts\": [0.5, 0.5]}, # Previous winner\n                {\"neurons\": [192, 192], \"dropouts\": [0.5, 0.5]},\n                {\"neurons\": [256, 256], \"dropouts\": [0.5, 0.5]},\n\n                # --- Two Layer Tapering Focus ---\n                {\"neurons\": [128, 32], \"dropouts\": [0.5, 0.3]},\n                {\"neurons\": [128, 64], \"dropouts\": [0.5, 0.4]},\n                {\"neurons\": [256, 64], \"dropouts\": [0.5, 0.4]},\n                {\"neurons\": [256, 128], \"dropouts\": [0.5, 0.4]},\n                {\"neurons\": [512, 128], \"dropouts\": [0.5, 0.4]},\n\n                # --- Three Layer Focus ---\n                {\"neurons\": [64, 64, 64], \"dropouts\": [0.4, 0.4, 0.4]},\n                {\"neurons\": [128, 128, 128], \"dropouts\": [0.5, 0.5, 0.5]},\n                {\"neurons\": [128, 64, 32], \"dropouts\": [0.5, 0.4, 0.3]},\n                {\"neurons\": [256, 128, 64], \"dropouts\": [0.5, 0.4, 0.3]},\n                {\"neurons\": [256, 64, 32], \"dropouts\": [0.5, 0.4, 0.3]},\n                {\"neurons\": [512, 128, 32], \"dropouts\": [0.5, 0.4, 0.3]},\n            ],\n\n            'Medium': [ # For Tg, Density. Balanced complexity.\n                # --- Two Layer Focus ---\n                {\"neurons\": [256, 64], \"dropouts\": [0.4, 0.3]},\n                {\"neurons\": [256, 128], \"dropouts\": [0.4, 0.3]},\n                {\"neurons\": [512, 64], \"dropouts\": [0.5, 0.3]},\n                {\"neurons\": [512, 128], \"dropouts\": [0.5, 0.4]}, # Previous winner\n                {\"neurons\": [512, 256], \"dropouts\": [0.5, 0.4]},\n                {\"neurons\": [1024, 128], \"dropouts\": [0.5, 0.4]},\n                {\"neurons\": [1024, 256], \"dropouts\": [0.5, 0.4]},\n                {\"neurons\": [256, 256], \"dropouts\": [0.4, 0.4]},\n                {\"neurons\": [512, 512], \"dropouts\": [0.5, 0.5]},\n\n                # --- Three Layer Focus ---\n                {\"neurons\": [256, 128, 64], \"dropouts\": [0.4, 0.3, 0.2]},\n                {\"neurons\": [512, 128, 64], \"dropouts\": [0.5, 0.4, 0.3]},\n                {\"neurons\": [512, 256, 64], \"dropouts\": [0.5, 0.4, 0.2]},\n                {\"neurons\": [512, 256, 128], \"dropouts\": [0.5, 0.4, 0.3]}, # Previous winner\n                {\"neurons\": [1024, 256, 64], \"dropouts\": [0.5, 0.4, 0.3]},\n                {\"neurons\": [1024, 512, 128], \"dropouts\": [0.5, 0.4, 0.3]},\n                {\"neurons\": [1024, 512, 256], \"dropouts\": [0.5, 0.4, 0.3]},\n                {\"neurons\": [256, 256, 256], \"dropouts\": [0.4, 0.4, 0.4]},\n                {\"neurons\": [512, 512, 512], \"dropouts\": [0.5, 0.5, 0.5]},\n\n                # --- Four Layer Focus ---\n                {\"neurons\": [512, 256, 128, 64], \"dropouts\": [0.5, 0.4, 0.3, 0.2]},\n                {\"neurons\": [1024, 512, 256, 128], \"dropouts\": [0.5, 0.4, 0.3, 0.2]},\n            ],\n\n            'High': [ # For FFV. Exploring width and depth.\n                # --- Refining Around Winner ([512, 256]) ---\n                {\"neurons\": [512, 128], \"dropouts\": [0.3, 0.2]},\n                {\"neurons\": [512, 256], \"dropouts\": [0.3, 0.2]}, # Previous winner\n                {\"neurons\": [512, 512], \"dropouts\": [0.3, 0.3]},\n                {\"neurons\": [1024, 256], \"dropouts\": [0.4, 0.3]},\n                {\"neurons\": [1024, 512], \"dropouts\": [0.4, 0.3]},\n                {\"neurons\": [1024, 1024], \"dropouts\": [0.4, 0.4]},\n                {\"neurons\": [2048, 512], \"dropouts\": [0.5, 0.4]},\n                {\"neurons\": [2048, 1024], \"dropouts\": [0.5, 0.4]},\n\n                # --- Three Layer Focus ---\n                {\"neurons\": [512, 256, 128], \"dropouts\": [0.3, 0.2, 0.2]},\n                {\"neurons\": [1024, 256, 64], \"dropouts\": [0.4, 0.3, 0.2]},\n                {\"neurons\": [1024, 512, 128], \"dropouts\": [0.4, 0.3, 0.2]},\n                {\"neurons\": [1024, 512, 256], \"dropouts\": [0.4, 0.3, 0.2]},\n                {\"neurons\": [2048, 512, 128], \"dropouts\": [0.5, 0.4, 0.3]},\n                {\"neurons\": [2048, 1024, 512], \"dropouts\": [0.5, 0.4, 0.3]},\n                {\"neurons\": [512, 512, 512], \"dropouts\": [0.3, 0.3, 0.3]},\n                {\"neurons\": [1024, 1024, 1024], \"dropouts\": [0.4, 0.4, 0.4]},\n\n                # --- Four+ Layer Focus ---\n                {\"neurons\": [512, 256, 256, 128], \"dropouts\": [0.3, 0.2, 0.2, 0.1]},\n                {\"neurons\": [1024, 512, 256, 128], \"dropouts\": [0.4, 0.3, 0.2, 0.2]},\n                {\"neurons\": [1024, 512, 512, 256], \"dropouts\": [0.4, 0.3, 0.3, 0.2]},\n                {\"neurons\": [512, 512, 512, 512], \"dropouts\": [0.3, 0.3, 0.3, 0.3]},\n            ]\n        }\n        \n        # Determine which set of configs to use\n        if label in ['Rg', 'Tc']:\n            configs_to_search = search_configs_by_label['Low']\n            print(\"Using search space for LOW data targets.\")\n        elif label == 'FFV':\n            configs_to_search = search_configs_by_label['High']\n            print(\"Using search space for HIGH data targets.\")\n        else: # Tg, Density\n            configs_to_search = search_configs_by_label['Medium']\n            print(\"Using search space for MEDIUM data targets.\")\n\n        results = []\n        for i, cfg in enumerate(configs_to_search):\n            print(f\"\\n---> Searching config {i+1}/{len(configs_to_search)}: Neurons={cfg['neurons']}, Dropouts={cfg['dropouts']}\")\n            model = FeedforwardNet(input_dim, cfg[\"neurons\"], cfg[\"dropouts\"])\n            model = train_model(model, X_train, X_val, y_train, y_val, verbose=False) # Turn off verbose for cleaner search logs\n            \n            model.eval()\n            with torch.no_grad():\n                X_val_np = np.asarray(X_val) if isinstance(X_val, pd.DataFrame) else X_val\n                device = next(model.parameters()).device\n                X_val_tensor = torch.tensor(X_val_np, dtype=torch.float32).to(device)\n                y_pred = model(X_val_tensor).cpu().numpy().flatten()\n            \n            val_mae = mean_absolute_error(y_val, y_pred)\n            print(f\"     Resulting Val MAE: {val_mae:.6f}\")\n            results.append({\"neurons\": cfg[\"neurons\"], \"dropouts\": cfg[\"dropouts\"], \"val_mae\": val_mae})\n        \n        df = pd.DataFrame(results)\n        print(\"\\n--- Neural Network Search Results ---\")\n        print(df.sort_values(by='val_mae').to_string(index=False))\n        \n        df.to_csv(f\"nn_config_validation_mae_{label}.csv\", index=False)\n        print(f\"\\nSaved results to nn_config_validation_mae_{label}.csv\")\n        \n        best_row = df.loc[df['val_mae'].idxmin()]\n        print(f\"\\nBest config: neurons={best_row['neurons']}, dropouts={best_row['dropouts']}, Validation MAE: {best_row['val_mae']:.6f}\")\n        \n        config = best_row.to_dict()\n        print(\"Re-training best model on the full training data...\")\n    \n    else: # If not searching, use a single pre-defined configuration\n        best_configs = {\n            \"Tg\":      {\"neurons\": [256, 128, 64], \"dropouts\": [0.4, 0.3, 0.2]},\n            \"Density\": {\"neurons\": [256, 128, 64], \"dropouts\": [0.4, 0.3, 0.2]},\n            \"FFV\":     {\"neurons\": [512, 256, 128], \"dropouts\": [0.3, 0.2, 0.2]},\n            \"Tc\":      {\"neurons\": [128, 64], \"dropouts\": [0.4, 0.3]},\n            \"Rg\":      {\"neurons\": [128, 64], \"dropouts\": [0.4, 0.3]},\n        }\n        config = best_configs.get(label)\n        print(f\"Using pre-defined best config for {label}: Neurons={config['neurons']}, Dropouts={config['dropouts']}\")\n\n    # Final model training\n    best_model = FeedforwardNet(input_dim, config[\"neurons\"], config[\"dropouts\"])\n    show_model_summary(best_model, input_dim)\n    best_model = train_model(best_model, X_train, X_val, y_train, y_val, verbose=True)\n    return best_model\n\n# Utility to set random state everywhere\ndef set_global_random_seed(seed, config=None):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    if config is not None:\n        config.random_state = seed\n\nimport hashlib\n\ndef stable_hash(obj, max_value=1_000_000):\n    \"\"\"\n    Deterministic hash for objects (e.g. labels).\n    Always returns the same value across runs/machines.\n    \"\"\"\n    # Convert to string and encode\n    s = str(obj).encode(\"utf-8\")\n    # Use MD5 (fast & deterministic)\n    h = hashlib.md5(s).hexdigest()\n    # Convert hex digest to int and limit range\n    return int(h, 16) % max_value\n\ndef train_and_evaluate_models(label, X_main, y_main, splits, nfold, Config):\n    \"\"\"\n    Trains models for the given label using the specified configuration.\n    Returns: models, fold_maes, mean_fold_mae, std_fold_mae\n    \"\"\"\n    # Use a prime multiplier for folds\n    FOLD_PRIME = 9973   # a large prime\n\n    models = []\n    fold_maes = []\n    mean_fold_mae = None\n    std_fold_mae = None\n\n    # Use stacking only if enabled in config\n    if getattr(Config, 'use_stacking', False):\n        Model = train_with_stacking(label, X_main, y_main)\n        models.append(Model)\n        save_model(Model, label, 1, Config.model_name)\n    elif Config.model_name in ['autogluon']:\n        Model = train_with_autogluon(label, X_main, y_main)\n        models.append(Model)\n        # save_model(Model, label, 1, Config.model_name)\n    else:\n        for fold, (train_idx, val_idx) in enumerate(splits):\n            print(f\"\\n--- Fold {fold+1}/{nfold} ---\")\n            # Set a different random seed for each fold for best possible result\n            # Use a deterministic but varied seed: base + fold + hash(label)\n            base_seed = getattr(Config, 'random_state', 42)\n            label_hash = stable_hash(label)   # replaces abs(hash(label)) % 10000\n            fold_seed = base_seed + fold * FOLD_PRIME  + label_hash\n\n            set_global_random_seed(fold_seed, config=Config)\n\n            # Robustly handle both DataFrame and ndarray\n            if isinstance(X_main, np.ndarray):\n                X_train, X_val = X_main[train_idx], X_main[val_idx]\n            else:\n                X_train, X_val = X_main.iloc[train_idx], X_main.iloc[val_idx]\n            if isinstance(y_main, np.ndarray):\n                y_train, y_val = y_main[train_idx], y_main[val_idx]\n            else:\n                y_train, y_val = y_main.iloc[train_idx], y_main.iloc[val_idx]\n\n            if Config.model_name == 'xgb':\n                Model = train_with_xgb(label, X_train, y_train, X_val, y_val)\n            elif Config.model_name in ['catboost', 'lgbm', 'extratrees', 'randomforest', 'balancedrf', 'tabnet', 'hgbm', 'autogluon', 'nn']:\n                Model = train_with_other_models(Config.model_name, label, X_train, y_train, X_val, y_val)\n            else:\n                assert False, \"No model present. Set Config.use_train_with_xgb = True to train a model.\"\n\n            # Save model for later holdout prediction\n            models.append(Model)\n            save_model(Model, label, fold, Config.model_name)\n\n            # Predict on validation set for this fold\n            if hasattr(Model, 'forward') and not hasattr(Model, 'predict'):\n                Model.eval()\n                X_val_np = np.asarray(X_val) if isinstance(X_val, pd.DataFrame) else X_val\n                device = next(Model.parameters()).device\n                with torch.no_grad():\n                    X_val_tensor = torch.tensor(X_val_np, dtype=torch.float32).to(device)\n                    y_val_pred = Model(X_val_tensor).cpu().numpy().flatten()\n            else:\n                y_val_pred = Model.predict(X_val)\n\n            fold_mae = mean_absolute_error(y_val, y_val_pred)\n            print(f\"Fold {fold+1} MAE (on validation set): {fold_mae}\")\n            fold_maes.append(fold_mae)\n            # Save y_val, y_val_pred, and residuals in sorted order for each fold\n            residuals = y_val - y_val_pred\n            results_df = pd.DataFrame({\n                'y_val': y_val,\n                'y_val_pred': y_val_pred,\n                'residual': residuals\n            })\n            results_df = results_df.sort_values(by='residual', ascending=False).reset_index(drop=True)\n            os.makedirs(f'NeurIPS/fold_residuals/{label}', exist_ok=True)\n            results_df.to_csv(f'NeurIPS/fold_residuals/{label}/fold_{fold+1}_val_pred_residuals.csv', index=False)\n        mean_fold_mae = np.mean(fold_maes)\n        std_fold_mae = np.std(fold_maes)\n        print(f\"{label} 5-Fold CV mean_absolute_error (on validation sets): {mean_fold_mae} ± {std_fold_mae}\")\n\n    return models, fold_maes, mean_fold_mae, std_fold_mae\n\ndef save_feature_selection_info(label, kept_columns, least_important_features, correlated_features_dropped, scaler, X_holdout, y_holdout, median_values):\n    holdout_dir = f\"NeurIPS/feature_selection/{label}\"\n    os.makedirs(holdout_dir, exist_ok=True)\n    feature_info = {\n        \"kept_columns\": list(kept_columns),\n        \"least_important_features\": list(least_important_features),\n        \"correlated_features_dropped\": list(correlated_features_dropped),\n    }\n    # Save median_values if provided\n    if median_values is not None:\n        if hasattr(median_values, 'to_dict'):\n            feature_info[\"median_values\"] = median_values.to_dict()\n        else:\n            feature_info[\"median_values\"] = median_values\n\n    # Save X_holdout and y_holdout for this label\n    X_holdout_path = os.path.join(holdout_dir, \"X_holdout.csv\")\n    y_holdout_path = os.path.join(holdout_dir, \"y_holdout.csv\")\n    pd.DataFrame(X_holdout).to_csv(X_holdout_path, index=False)\n    pd.DataFrame({\"y_holdout\": y_holdout}).to_csv(y_holdout_path, index=False)\n\n    feature_info_path = os.path.join(holdout_dir, f\"{label}_feature_info.json\")\n    with open(feature_info_path, \"w\") as f:\n        json.dump(feature_info, f, indent=2)\n\n    # Save scaler object\n    scaler_path = os.path.join(holdout_dir, \"scaler.joblib\")\n    if scaler is not None:\n        joblib.dump(scaler, scaler_path)\n\ndef load_feature_selection_info(label, base_dir):\n    \"\"\"\n    Loads feature selection info saved by save_feature_selection_info for a given label.\n    Returns a dict with keys: kept_columns, least_important_features, correlated_features_dropped, scaler, X_holdout, y_holdout.\n    \"\"\"\n\n    holdout_dir = os.path.join(base_dir, f\"NeurIPS/feature_selection/{label}\")\n    feature_info_path = os.path.join(holdout_dir, f\"{label}_feature_info.json\")\n    X_holdout_path = os.path.join(holdout_dir, \"X_holdout.csv\")\n    y_holdout_path = os.path.join(holdout_dir, \"y_holdout.csv\")\n\n    if not os.path.exists(feature_info_path):\n        raise FileNotFoundError(f\"Feature info file not found: {feature_info_path}\")\n\n    with open(feature_info_path, \"r\") as f:\n        feature_info = json.load(f)\n\n    X_holdout = pd.read_csv(X_holdout_path)\n    y_holdout = pd.read_csv(y_holdout_path)[\"y_holdout\"].values\n\n    # Note: scaler is not restored as an object (only its params or type string is saved)\n    # If you need the actual scaler object, you must save it with joblib or pickle\n\n    # Load scaler object\n    scaler_path = os.path.join(holdout_dir, \"scaler.joblib\")\n    if os.path.exists(scaler_path):\n        scaler = joblib.load(scaler_path)\n    else:\n        scaler = None\n\n    # Try to load median_values if present in feature_info, else set to empty Series\n    if \"median_values\" in feature_info:\n        median_values = pd.Series(feature_info[\"median_values\"])\n    else:\n        median_values = pd.Series(dtype=float)\n    return {\n        \"kept_columns\": feature_info.get(\"kept_columns\", []),\n        \"least_important_features\": feature_info.get(\"least_important_features\", []),\n        \"correlated_features_dropped\": feature_info.get(\"correlated_features_dropped\", []),\n        \"X_holdout\": X_holdout,\n        \"y_holdout\": y_holdout,\n        \"scaler\": scaler,\n        \"median_values\": median_values\n    }\n\ndef load_models_for_label(label, models_dir=\"models\"):\n    \"\"\"\n    Loads all models for a given label from the specified directory.\n    Model filenames must start with the label (e.g., 'Tg_fold1_xgb.joblib').\n    Returns a list of loaded models.\n    \"\"\"\n\n    models = []\n    if not os.path.exists(models_dir):\n        print(f\"Models directory '{models_dir}' does not exist.\")\n        return models\n\n    # Match both .joblib and .pt (for torch) files\n    pattern_joblib = os.path.join(models_dir, f\"{label}_*.joblib\")\n    pattern_pt = os.path.join(models_dir, f\"{label}_*.pt\")\n    model_files = glob.glob(pattern_joblib) + glob.glob(pattern_pt)\n    if not model_files:\n        print(f\"No models found for label '{label}' in '{models_dir}'.\")\n        return models\n\n    for model_file in sorted(model_files):\n        if model_file.endswith(\".joblib\"):\n            try:\n                model = joblib.load(model_file)\n                models.append(model)\n            except Exception as e:\n                print(f\"Failed to load model {model_file}: {e}\")\n        elif model_file.endswith(\".pt\"):\n            # Torch model loading requires model class and architecture\n            print(f\"Skipping torch model {model_file} (requires model class definition).\")\n            # You can implement torch loading here if needed\n    print(f\"Loaded {len(models)} models for label '{label}'.\")\n    return models\n\noutput_df = pd.DataFrame({\n    'id': test_ids\n})\n\n# --- Store and display mean_absolute_error for each label ---\nmae_results = []\n\ndef prepare_label_data(label, subtables, Config):\n    print(f\"Processing label: {label}\")\n    print(subtables[label].head())\n    print(subtables[label].shape)\n    original_smiles = subtables[label]['SMILES'].tolist()\n    original_labels = subtables[label][label].values\n\n    # Canonicalize SMILES and deduplicate at molecule level before augmentation\n    canonical_smiles = [get_canonical_smiles(s) for s in original_smiles]\n    smiles_label_df = pd.DataFrame({\n        'SMILES': canonical_smiles,\n        'label': original_labels\n    })\n    before_dedup = len(smiles_label_df)\n    smiles_label_df = smiles_label_df.drop_duplicates(subset=['SMILES'], keep='first').reset_index(drop=True)\n    after_dedup = len(smiles_label_df)\n    num_dropped = before_dedup - after_dedup\n    print(f\"Dropped {num_dropped} duplicate SMILES rows for {label} before augmentation.\")\n    original_smiles = smiles_label_df['SMILES'].tolist()\n    original_labels = smiles_label_df['label'].values\n\n    if Config.use_augmentation and not Config.debug:\n        print(f\"SMILES before augmentation: {len(original_smiles)}\")\n        smiles_aug, labels_aug = augment_smiles_dataset(original_smiles, original_labels, num_augments=1)\n        print(f\"SMILES after augmentation: {len(smiles_aug)} (increase: {len(smiles_aug) - len(original_smiles)})\")\n        original_smiles, original_labels = smiles_aug, labels_aug\n\n    # After augmentation, deduplicate again at molecule level (canonical SMILES)\n    canonical_smiles_aug = [get_canonical_smiles(s) for s in original_smiles]\n    smiles_label_aug_df = pd.DataFrame({\n        'SMILES': canonical_smiles_aug,\n        'label': original_labels\n    })\n    smiles_label_aug_df = smiles_label_aug_df.drop_duplicates(subset=['SMILES'], keep='first').reset_index(drop=True)\n    original_smiles = smiles_label_aug_df['SMILES'].tolist()\n    original_labels = smiles_label_aug_df['label'].values\n\n    fp_df, descriptor_df, valid_smiles, invalid_indices = smiles_to_combined_fingerprints_with_descriptors(original_smiles)\n\n    print(f\"Invalid indices for {label}: {invalid_indices}\")\n    y = np.delete(original_labels, invalid_indices)\n    print(fp_df.shape)\n    fp_df.reset_index(drop=True, inplace=True)\n\n    if not descriptor_df.empty:\n        X = pd.DataFrame(descriptor_df)\n        X, kept_columns, scaler, median_values = preprocess_numerical_features(X, label)\n        X.reset_index(drop=True, inplace=True)\n        if not fp_df.empty:\n            X = pd.concat([X, fp_df], axis=1)\n    else:\n        kept_columns = []\n        scaler = None\n        X = fp_df\n\n    # Remove duplicate rows in X and corresponding values in y (feature-level duplicates)\n    X_dup = X.duplicated(keep='first')\n    if X_dup.any():\n        print(f\"Found {X_dup.sum()} duplicate rows in X for {label}, removing them.\")\n        X = X[~X_dup]\n        y = y[~X_dup]\n    print(f\"After concat: {X.shape}\")\n    # Fill NaN in train with median from train\n    # Only fill NaN with median if using neural network\n    if Config.model_name == 'nn':\n        X = X.fillna(median_values)\n    check_inf_nan(X, y, label)\n\n    # display_outlier_summary(y, X=X, name=label, z_thresh=3, iqr_factor=1.5, iso_contamination=0.01, lof_contamination=0.01)\n\n    # Drop least important features from X and test\n\n    least_important_features = []\n    if getattr(Config, 'use_least_important_features_all_methods', False):\n        for i in range(4):\n            print(f\"Iteration {i+1} for least important feature removal on {label}\")\n            least_important_feature = get_least_important_features_all_methods(X, y, label, model_name=Config.model_name)\n            least_important_features.extend(least_important_feature)\n            if len(least_important_feature) > 0:\n                print(f\"label: {label} Dropping least important features: {least_important_feature}\")\n                X = X.drop(columns=least_important_feature)\n                print(f\"After dropping least important features: {X.shape}\")\n\n    check_inf_nan(X, y, label)\n    # Drop highly correlated features using label-specific correlation threshold from Config\n    correlation_threshold = Config.correlation_thresholds.get(label, 1.0)\n    if correlation_threshold < 1.0:\n        X, correlated_features_dropped = drop_correlated_features(pd.DataFrame(X), threshold=correlation_threshold)\n    else:\n        correlated_features_dropped = []\n\n    print(f\"After correlation cut (threshold={correlation_threshold}): {X.shape}, dropped columns: {correlated_features_dropped}\")\n    print(f\"After dropping correlated features: {X.shape}\")\n    check_inf_nan(X, y, label)\n\n    if getattr(Config, 'use_variance_threshold', False):\n        threshold = 0.01\n        selector = VarianceThreshold(threshold=threshold)\n        X_sel = selector.fit_transform(X)\n        # Get mask of selected features\n        selected_cols_variance = X.columns[selector.get_support()]\n        # Convert back to DataFrame with column names\n        X = pd.DataFrame(X_sel, columns=selected_cols_variance, index=X.index)\n        print(f\"After variance cut: {X.shape}\")\n        print(f'Type of X: {type(X)}')\n\n    if Config.add_gaussian and not Config.debug:\n        n_samples = 1000\n        X, y = augment_dataset(X, y, n_samples=n_samples)\n        print(f\"After augment cut: {X.shape}\")\n\n    # --- Hold out 10% for final MAE calculation ---\n    # X_main, X_holdout, y_main, y_holdout = train_test_split(X, y, test_size=0.1, random_state=Config.random_state)\n    # Bin y for stratification\n    y_bins = pd.qcut(y, q=5, duplicates='drop', labels=False)\n    X_main, X_holdout, y_main, y_holdout = train_test_split(X, y, test_size=0.10, random_state=Config.random_state, stratify=y_bins)\n    if Config.useAllDataForTraining == True:\n        X_main = X\n        y_main = y\n    # --- Optionally apply PCA ---\n    if getattr(Config, 'use_pca', False):\n        X_main, X_holdout, pca = apply_pca(X_main, X_holdout, verbose=True)\n    else:\n        pca = None\n\n    # --- Cross-Validation or Single Split (for speed) ---\n    fold_maes = []\n    test_preds = []\n    val_preds = np.zeros(len(y_main))\n    if getattr(Config, 'use_cross_validation', True):\n        nfold = 10\n        from sklearn.model_selection import StratifiedKFold\n        skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=Config.random_state)\n        # For regression, bin y_main for stratification\n        y_bins = pd.qcut(y_main, q=nfold, duplicates='drop', labels=False)\n        splits = skf.split(X_main, y_bins)\n    else:\n        # Use a single split: 80% train, 20% val\n        train_idx, val_idx = train_test_split(\n            np.arange(len(X_main)), test_size=0.2, random_state=Config.random_state\n        )\n        splits = [(train_idx, val_idx)]\n        nfold = 1\n\n    return {\n        \"X_main\": X_main,\n        \"X_holdout\": X_holdout,\n        \"y_main\": y_main,\n        \"y_holdout\": y_holdout,\n        \"kept_columns\": kept_columns,\n        \"scaler\": scaler,\n        \"median_values\": median_values,\n        \"least_important_features\": least_important_features,\n        \"correlated_features_dropped\": correlated_features_dropped,\n        \"selector\": selector if getattr(Config, 'use_variance_threshold', False) else None,\n        \"selected_cols_variance\": selected_cols_variance if getattr(Config, 'use_variance_threshold', False) else None,\n        \"pca\": pca,\n        \"splits\": splits,\n        \"nfold\": nfold,\n        \"fold_maes\": fold_maes,\n        \"test_preds\": test_preds,\n        \"val_preds\": val_preds\n    }\n\ndef load_label_data(label, model_dir=None):\n    if model_dir is not None:\n        # Load model and data for the specified label\n        model_path = os.path.join(model_dir, f\"{label}_model.pkl\")\n        data_path = os.path.join(model_dir, f\"{label}_data.pkl\")\n        model = joblib.load(model_path)\n        data = joblib.load(data_path)\n        return model, data\n    return None, None\n\ndef train_or_predict(train_model=True, model_dir=None):\n    for label in labels:\n        if train_model:\n            print(f\"\\n=== Training/Predicting for label: {label} ===\")\n            label_data = prepare_label_data(label, subtables, config)\n            X_main = label_data[\"X_main\"]\n            X_holdout = label_data[\"X_holdout\"]\n            y_main = label_data[\"y_main\"]\n            y_holdout = label_data[\"y_holdout\"]\n            kept_columns = label_data[\"kept_columns\"]\n            scaler = label_data[\"scaler\"]\n            median_values = label_data[\"median_values\"]\n            least_important_features = label_data[\"least_important_features\"]\n            correlated_features_dropped = label_data[\"correlated_features_dropped\"]\n            selector = label_data[\"selector\"]\n            selected_cols_variance = label_data[\"selected_cols_variance\"]\n            pca = label_data[\"pca\"]\n            splits = label_data[\"splits\"]\n            nfold = label_data[\"nfold\"]\n            fold_maes = label_data[\"fold_maes\"]\n            test_preds = label_data[\"test_preds\"]\n            val_preds = label_data[\"val_preds\"]\n\n            # --- Save feature selection info for this label ---\n            save_feature_selection_info(label, kept_columns, least_important_features, correlated_features_dropped, scaler, X_holdout, y_holdout, median_values)  \n            os.makedirs('models', exist_ok=True)\n            models = []\n\n            # labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\n            # --- Hyperparameter tuning for this label ---\n            if Config.enable_param_tuning:\n                if label == 'Tg':\n                    db_path = f\"xgb_tuning_{label}.db\"\n                    init_xgb_tuning_db(db_path)\n                    # Example param_grid (customize as needed)\n                    param_grid = {\n                        'n_estimators': [3000],\n                        'max_depth': [4, 5, 6, 7],\n                        'learning_rate': [0.001, 0.01, 0.06, 0.1],\n                        'subsample': [0.6, 0.8, 1.0],\n                        'colsample_bytree': [0.8, 1.0],\n                        'gamma': [0, 0.1],\n                        'reg_lambda': [1.0, 5.0, 10.0],\n                        'early_stopping_rounds': [50],\n                        'objective': [\"reg:squarederror\"],\n                        'eval_metric': [\"rmse\"]\n                    }\n                    # You must define X_train and y_train for tuning here\n                    xgb_grid_search_with_db(X_main, y_main, param_grid, db_path=db_path)\n                else:\n                    continue\n            # FIXME save features_keep\n            # Ensure only features present in X_main are kept\n\n            # if label in ['Tg', 'Tc', 'Density', 'Rg']:\n            #     features_keep = select_features_with_lasso(X_main, y_main, label)\n            #     X_main = X_main[features_keep]\n            models, fold_maes, mean_fold_mae, std_fold_mae = train_and_evaluate_models(label, X_main, y_main, splits, nfold, Config)\n        else:\n            print(f\"\\n=== Loading models and data for label: {label} ===\")\n            mean_fold_mae, std_fold_mae = None, None\n            feature_info = load_feature_selection_info(label, model_dir)\n            kept_columns = feature_info[\"kept_columns\"]\n            least_important_features = feature_info[\"least_important_features\"]\n            correlated_features_dropped = feature_info[\"correlated_features_dropped\"]\n            scaler = feature_info.get(\"scaler\", None)\n            X_holdout = feature_info[\"X_holdout\"]\n            y_holdout = feature_info[\"y_holdout\"]\n            median_values = feature_info[\"median_values\"]\n            selector = None\n            selected_cols_variance = None\n            pca = None\n\n            models = load_models_for_label(label, os.path.join(model_dir, 'models'))\n            test_preds = []\n\n        # Prepare test set once\n        fp_df, descriptor_df, valid_smiles, invalid_indices = smiles_to_combined_fingerprints_with_descriptors(test_smiles)\n        # median_values = label_data[\"median_values\"]\n        if not descriptor_df.empty:\n            # Safely align test columns to training columns. \n            # This adds any missing columns and fills them with NaN.\n            descriptor_df = descriptor_df.reindex(columns=kept_columns)\n            if Config.model_name == 'nn':\n                # Fill NaN in test with median from train\n                descriptor_df = descriptor_df.fillna(median_values)\n            # Scale test set using the same scaler and kept_columns, then convert to DataFrame\n            if getattr(Config, 'use_standard_scaler', False) and scaler is not None:\n                descriptor_df = pd.DataFrame(scaler.transform(descriptor_df), columns=kept_columns, index=descriptor_df.index)\n            descriptor_df.reset_index(drop=True, inplace=True)\n            if not fp_df.empty:\n                fp_df = fp_df.reset_index(drop=True)\n                test = pd.concat([descriptor_df, fp_df], axis=1)\n            else:\n                test = descriptor_df\n        else:\n            test = fp_df\n\n        if len(least_important_features) > 0:\n            test = test.drop(columns=least_important_features)\n        if len(correlated_features_dropped) > 0:\n            print(f\"Dropping correlated columns from test: {correlated_features_dropped}\")\n            test = test.drop(correlated_features_dropped, axis=1, errors='ignore')\n        if getattr(Config, 'use_variance_threshold', False):\n            test_sel = selector.transform(test)\n            # Convert back to DataFrame\n            test = pd.DataFrame(test_sel, columns=selected_cols_variance, index=test.index)\n        # Optionally apply PCA to test set if enabled\n        if getattr(Config, 'use_pca', False):\n            test = pca.transform(test)\n        # if label in ['Tg', 'Tc', 'Density', 'Rg']:\n        #     X_holdout = X_holdout[features_keep]\n        #     test = test[features_keep]\n        # --- Holdout set evaluation with all trained models ---\n        holdout_maes = []\n        for i, Model in enumerate(models):\n            is_torch_model = hasattr(Model, 'forward') and not hasattr(Model, 'predict')\n\n            if is_torch_model:\n                Model.eval()\n                X_holdout_np = np.asarray(X_holdout) if isinstance(X_holdout, pd.DataFrame) else X_holdout\n                test_np = np.asarray(test) if isinstance(test, pd.DataFrame) else test\n                device = next(Model.parameters()).device\n                with torch.no_grad():\n                    X_holdout_tensor = torch.tensor(X_holdout_np, dtype=torch.float32).to(device)\n                    test_tensor = torch.tensor(test_np, dtype=torch.float32).to(device)\n                    y_holdout_pred = Model(X_holdout_tensor).detach().cpu().numpy().flatten()\n                    y_test_pred = Model(test_tensor).detach().cpu().numpy().flatten()\n            else:\n                y_holdout_pred = Model.predict(X_holdout)\n                y_test_pred = Model.predict(test)\n\n            holdout_mae = mean_absolute_error(y_holdout, y_holdout_pred)\n            print(f\"Model {i+1} holdout MAE: {holdout_mae}\")\n            holdout_maes.append(holdout_mae)\n\n            if isinstance(y_test_pred, pd.Series):\n                y_test_pred = y_test_pred.values.flatten()\n            else:\n                y_test_pred = y_test_pred.flatten()        \n            test_preds.append(y_test_pred)\n\n        mean_holdout_mae = np.mean(holdout_maes)\n        std_holdout_mae = np.std(holdout_maes)\n        print(f\"{label} Holdout MAE (mean ± std over all models): {mean_holdout_mae:.5f} ± {std_holdout_mae:.5f}\")\n\n        mae_results.append({\n            'label': label,\n            'fold_mae_mean': mean_fold_mae,\n            'fold_mae_std': std_fold_mae,\n            'holdout_mae_mean': mean_holdout_mae,\n            'holdout_mae_std': std_holdout_mae\n        })\n\n        # Average test predictions across folds\n        test_preds = np.array(test_preds)\n        y_pred = np.mean(test_preds, axis=0)\n        print(y_pred)\n        new_column_name = label\n        output_df[new_column_name] = y_pred\n\n    # Save MAE results to CSV and display\n    mae_df = pd.DataFrame(mae_results)\n    mae_df.to_csv('NeurIPS/mae_results.csv', index=False)\n    print(\"\\nMean Absolute Error for each label:\")\n    print(mae_df)\n\n# train_or_predict()\n\n# output_df.to_csv('submission.csv', index=False)\n\n\n# output_df = pd.DataFrame({\n#     'id': test_ids\n# })\n# MODEL_DIR1 = '/kaggle/input/neurips-2025/nn_v4'\n# train_or_predict(train_model=False, model_dir=MODEL_DIR1)\n# # output_dfs.append(train_or_predict(output_df, train_model=False, model_dir=MODEL_DIR1))\n# print(output_df)\n# output_dfs.append(output_df.copy())\n\n\noutput_df = pd.DataFrame({\n    'id': test_ids\n})\nMODEL_DIR1 = '/kaggle/input/neurips-2025/xgb_v3'\ntrain_or_predict(train_model=False, model_dir=MODEL_DIR1)\nprint(output_df)\noutput_dfs.append(output_df.copy())\n","metadata":{"execution":{"iopub.execute_input":"2025-09-12T13:02:09.011127Z","iopub.status.busy":"2025-09-12T13:02:09.010901Z","iopub.status.idle":"2025-09-12T13:02:35.020346Z","shell.execute_reply":"2025-09-12T13:02:35.019422Z"},"papermill":{"duration":26.025425,"end_time":"2025-09-12T13:02:35.02146","exception":false,"start_time":"2025-09-12T13:02:08.996035","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GNN","metadata":{"papermill":{"duration":0.009253,"end_time":"2025-09-12T13:02:35.040805","exception":false,"start_time":"2025-09-12T13:02:35.031552","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install /kaggle/input/torch-geometric-2-6-1/torch_geometric-2.6.1-py3-none-any.whl","metadata":{"execution":{"iopub.execute_input":"2025-09-12T13:02:35.060367Z","iopub.status.busy":"2025-09-12T13:02:35.060138Z","iopub.status.idle":"2025-09-12T13:02:39.117979Z","shell.execute_reply":"2025-09-12T13:02:39.117187Z"},"papermill":{"duration":4.069027,"end_time":"2025-09-12T13:02:39.119345","exception":false,"start_time":"2025-09-12T13:02:35.050318","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.mixture import GaussianMixture\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nimport joblib\nimport os\nimport torch\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader as PyGDataLoader\nfrom torch_geometric.nn import GCNConv, GINEConv, global_mean_pool, global_max_pool\nimport torch.nn.functional as F\nimport warnings\nimport json\nimport torch\nfrom sklearn.preprocessing import RobustScaler\nimport json\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GINEConv, global_mean_pool\n\nRDKIT_AVAILABLE = True\nTARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\nos.makedirs(\"NeurIPS\", exist_ok=True)\nclass Config:\n    debug = False\n    use_cross_validation = True  # Set to False to use a single split for speed\n    use_external_data = True  # Set to True to use external datasets\n    random_state = 42\n\n# Create a single config instance to use everywhere\nconfig = Config()\n\n\"\"\"\nLoad competition data with complete filtering of problematic polymer notation\n\"\"\"\nprint(\"Loading competition data...\")\ntrain = pd.read_csv(BASE_PATH + 'train.csv')\ntest = pd.read_csv(BASE_PATH + 'test.csv')\n\nif config.debug:\n    print(\"   Debug mode: sampling 1000 training examples\")\n    train = train.sample(n=1000, random_state=42).reset_index(drop=True)\n\nprint(f\"Training data shape: {train.shape}, Test data shape: {test.shape}\")\n\ndef clean_and_validate_smiles(smiles):\n    \"\"\"Completely clean and validate SMILES, removing all problematic patterns\"\"\"\n    if not isinstance(smiles, str) or len(smiles) == 0:\n        return None\n    \n    # List of all problematic patterns we've seen\n    bad_patterns = [\n        '[R]', '[R1]', '[R2]', '[R3]', '[R4]', '[R5]', \n        \"[R']\", '[R\"]', 'R1', 'R2', 'R3', 'R4', 'R5',\n        # Additional patterns that cause issues\n        '([R])', '([R1])', '([R2])', \n    ]\n    \n    for pattern in bad_patterns:\n        if pattern in smiles:\n            return None\n    \n    # Additional check: if it contains ] followed by [ without valid atoms, likely polymer notation\n    if '][' in smiles and any(x in smiles for x in ['[R', 'R]']):\n        return None\n    \n    # Try to parse with RDKit if available\n    if RDKIT_AVAILABLE:\n        try:\n            mol = Chem.MolFromSmiles(smiles)\n            if mol is not None:\n                return Chem.MolToSmiles(mol, canonical=True)\n            else:\n                return None\n        except:\n            return None\n    \n    # If RDKit not available, return cleaned SMILES\n    return smiles\n\n# Clean and validate all SMILES\nprint(\"Cleaning and validating SMILES...\")\ntrain['SMILES'] = train['SMILES'].apply(clean_and_validate_smiles)\ntest['SMILES'] = test['SMILES'].apply(clean_and_validate_smiles)\n\n# Remove invalid SMILES\ninvalid_train = train['SMILES'].isnull().sum()\ninvalid_test = test['SMILES'].isnull().sum()\n\nprint(f\"   Removed {invalid_train} invalid SMILES from training data\")\nprint(f\"   Removed {invalid_test} invalid SMILES from test data\")\n\ntrain = train[train['SMILES'].notnull()].reset_index(drop=True)\ntest = test[test['SMILES'].notnull()].reset_index(drop=True)\n\nprint(f\"   Final training samples: {len(train)}\")\nprint(f\"   Final test samples: {len(test)}\")\n\ndef add_extra_data_clean(df_train, df_extra, target):\n    \"\"\"Add external data with thorough SMILES cleaning\"\"\"\n    n_samples_before = len(df_train[df_train[target].notnull()])\n    \n    print(f\"      Processing {len(df_extra)} {target} samples...\")\n    \n    # Clean external SMILES\n    df_extra['SMILES'] = df_extra['SMILES'].apply(clean_and_validate_smiles)\n    \n    # Remove invalid SMILES and missing targets\n    before_filter = len(df_extra)\n    df_extra = df_extra[df_extra['SMILES'].notnull()]\n    df_extra = df_extra.dropna(subset=[target])\n    after_filter = len(df_extra)\n    \n    print(f\"      Kept {after_filter}/{before_filter} valid samples\")\n    \n    if len(df_extra) == 0:\n        print(f\"      No valid data remaining for {target}\")\n        return df_train\n    \n    # Group by canonical SMILES and average duplicates\n    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n    \n    cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n    unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n\n    # Fill missing values\n    filled_count = 0\n    for smile in df_train[df_train[target].isnull()]['SMILES'].tolist():\n        if smile in cross_smiles:\n            df_train.loc[df_train['SMILES']==smile, target] = \\\n                df_extra[df_extra['SMILES']==smile][target].values[0]\n            filled_count += 1\n    \n    # Add unique SMILES\n    extra_to_add = df_extra[df_extra['SMILES'].isin(unique_smiles_extra)].copy()\n    if len(extra_to_add) > 0:\n        for col in TARGETS:\n            if col not in extra_to_add.columns:\n                extra_to_add[col] = np.nan\n        \n        extra_to_add = extra_to_add[['SMILES'] + TARGETS]\n        df_train = pd.concat([df_train, extra_to_add], axis=0, ignore_index=True)\n\n    n_samples_after = len(df_train[df_train[target].notnull()])\n    print(f'      {target}: +{n_samples_after-n_samples_before} samples, +{len(unique_smiles_extra)} unique SMILES')\n    print(f\"      Filled {filled_count} missing entries in train for {target}\")\n    print(f\"      Added {len(extra_to_add)} new entries for {target}\")\n    return df_train\n\n# Load external datasets with robust error handling\nprint(\"\\n📂 Loading external datasets...\")\n\nexternal_datasets = []\n\n# Function to safely load datasets\ndef safe_load_dataset(path, target, processor_func, description):\n    try:\n        if path.endswith('.xlsx'):\n            data = pd.read_excel(path)\n        else:\n            data = pd.read_csv(path)\n        \n        data = processor_func(data)\n        external_datasets.append((target, data))\n        print(f\"   ✅ {description}: {len(data)} samples\")\n        return True\n    except Exception as e:\n        print(f\"   ⚠️ {description} failed: {str(e)[:100]}\")\n        return False\n\n# Load each dataset\nsafe_load_dataset(\n    '/kaggle/input/tc-smiles/Tc_SMILES.csv',\n    'Tc',\n    lambda df: df.rename(columns={'TC_mean': 'Tc'}),\n    'Tc data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv',\n    'Tg', \n    lambda df: df[['SMILES', 'Tg']] if 'Tg' in df.columns else df,\n    'TgSS enriched data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv',\n    'Tg',\n    lambda df: df[['SMILES', 'Tg (C)']].rename(columns={'Tg (C)': 'Tg'}),\n    'JCIM Tg data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/smiles-extra-data/data_tg3.xlsx',\n    'Tg',\n    lambda df: df.rename(columns={'Tg [K]': 'Tg'}).assign(Tg=lambda x: x['Tg'] - 273.15),\n    'Xlsx Tg data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/smiles-extra-data/data_dnst1.xlsx',\n    'Density',\n    lambda df: df.rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]\n                .query('SMILES.notnull() and Density.notnull() and Density != \"nylon\"')\n                .assign(Density=lambda x: x['Density'].astype(float) - 0.118),\n    'Density data'\n)\n\nsafe_load_dataset(\n    BASE_PATH + 'train_supplement/dataset4.csv',\n    'FFV', \n    lambda df: df[['SMILES', 'FFV']] if 'FFV' in df.columns else df,\n    'dataset 4'\n)\n\n# Integrate external data\nprint(\"\\n🔄 Integrating external data...\")\ntrain_extended = train[['SMILES'] + TARGETS].copy()\n\nif getattr(config, \"use_external_data\", True) and  not config.debug:\n    for target, dataset in external_datasets:\n        print(f\"   Processing {target} data...\")\n        train_extended = add_extra_data_clean(train_extended, dataset, target)\n\nprint(f\"\\n📊 Final training data:\")\nprint(f\"   Original samples: {len(train)}\")\nprint(f\"   Extended samples: {len(train_extended)}\")\nprint(f\"   Gain: +{len(train_extended) - len(train)} samples\")\n\nfor target in TARGETS:\n    count = train_extended[target].notna().sum()\n    original_count = train[target].notna().sum() if target in train.columns else 0\n    gain = count - original_count\n    print(f\"   {target}: {count:,} samples (+{gain})\")\n\nprint(f\"\\n✅ Data integration complete with clean SMILES!\")\n\ndef separate_subtables(train_df):\n    labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n    subtables = {}\n    for label in labels:\n        # Filter out NaNs, select columns, reset index\n        subtables[label] = train_df[train_df[label].notna()][['SMILES', label]].reset_index(drop=True)\n\n    return subtables\n\ndef augment_smiles_dataset(smiles_list, labels, num_augments=3):\n    \"\"\"\n    Augments a list of SMILES strings by generating randomized versions.\n\n    Parameters:\n        smiles_list (list of str): Original SMILES strings.\n        labels (list or np.array): Corresponding labels.\n        num_augments (int): Number of augmentations per SMILES.\n\n    Returns:\n        tuple: (augmented_smiles, augmented_labels)\n    \"\"\"\n    augmented_smiles = []\n    augmented_labels = []\n\n    for smiles, label in zip(smiles_list, labels):\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None:\n            continue\n        # Add original\n        augmented_smiles.append(smiles)\n        augmented_labels.append(label)\n        # Add randomized versions\n        for _ in range(num_augments):\n            rand_smiles = Chem.MolToSmiles(mol, doRandom=True)\n            augmented_smiles.append(rand_smiles)\n            augmented_labels.append(label)\n\n    return augmented_smiles, np.array(augmented_labels)\n\nrequired_descriptors = {'graph_diameter','num_cycles','avg_shortest_path','MolWt', 'LogP', 'TPSA', 'RotatableBonds', 'NumAtoms'}\n\ndef augment_dataset(X, y, n_samples=1000, n_components=5, random_state=None):\n    \"\"\"\n    Augments a dataset using Gaussian Mixture Models.\n\n    Parameters:\n    - X: pd.DataFrame or np.ndarray — feature matrix\n    - y: pd.Series or np.ndarray — target values\n    - n_samples: int — number of synthetic samples to generate\n    - n_components: int — number of GMM components\n    - random_state: int — random seed for reproducibility\n\n    Returns:\n    - X_augmented: pd.DataFrame — augmented feature matrix\n    - y_augmented: pd.Series — augmented target values\n    \"\"\"\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n    elif not isinstance(X, pd.DataFrame):\n        raise ValueError(\"X must be a pandas DataFrame or a NumPy array\")\n\n    X.columns = X.columns.astype(str)\n\n    if isinstance(y, np.ndarray):\n        y = pd.Series(y)\n    elif not isinstance(y, pd.Series):\n        raise ValueError(\"y must be a pandas Series or a NumPy array\")\n\n    df = X.copy()\n    df['Target'] = y.values\n\n    gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n    gmm.fit(df)\n\n    synthetic_data, _ = gmm.sample(n_samples)\n    synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n\n    augmented_df = pd.concat([df, synthetic_df], ignore_index=True)\n\n    X_augmented = augmented_df.drop(columns='Target')\n    y_augmented = augmented_df['Target']\n\n    return X_augmented, y_augmented\n\n\ntrain_df=train_extended\ntest_df=test\nsubtables = separate_subtables(train_df)\n\ntest_smiles = test_df['SMILES'].tolist()\ntest_ids = test_df['id'].values\nlabels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\n# ------------------------------------------------------------------\n# --- GNN MODEL AND DATA PREPARATION ---\n# ------------------------------------------------------------------\n\n# A dictionary to map atom symbols to integer indices for the GNN\nATOM_MAP = {\n    'C': 0, 'N': 1, 'O': 2, 'F': 3, 'P': 4, 'S': 5, 'Cl': 6, 'Br': 7, 'I': 8, 'H': 9,\n    # --- NEWLY ADDED SYMBOLS ---\n    'Si': 10, # Silicon\n    'Na': 11, # Sodium\n    '*' : 12, # Wildcard atom\n    # --- NEWLY ADDED SYMBOLS ---\n    'B': 13,  # Boron\n    'Ge': 14, # Germanium\n    'Sn': 15, # Tin\n    'Se': 16, # Selenium\n    'Te': 17, # Tellurium\n    'Ca': 18, # Calcium\n    'Cd': 19, # Cadmium\n}\n\ndef smiles_to_graph(smiles_str: str, y_val=None):\n    \"\"\"\n    Converts a SMILES string to a graph, adding selected global\n    molecular features to each node's feature vector.\n    \"\"\"\n    try:\n        mol = Chem.MolFromSmiles(smiles_str)\n        if mol is None: return None\n\n        # 1. Calculate global features once per molecule\n        global_features = [\n            Descriptors.MolWt(mol),\n            Descriptors.TPSA(mol),\n            Descriptors.NumRotatableBonds(mol),\n            Descriptors.MolLogP(mol)\n        ]\n\n        node_features = []\n        for atom in mol.GetAtoms():\n            # Initialize atom-specific features (one-hot encoding)\n            atom_features = [0] * len(ATOM_MAP)\n            symbol = atom.GetSymbol()\n            if symbol in ATOM_MAP:\n                atom_features[ATOM_MAP[symbol]] = 1\n\n            # Add other standard atom features\n            atom_features.extend([\n                atom.GetAtomicNum(),\n                atom.GetTotalDegree(),\n                atom.GetFormalCharge(),\n                atom.GetTotalNumHs(),\n                int(atom.GetIsAromatic())\n            ])\n            \n            # 2. Append the global features to each atom's feature vector\n            atom_features.extend(global_features)\n            \n            node_features.append(atom_features)\n        \n        if not node_features: return None\n        x = torch.tensor(node_features, dtype=torch.float)\n\n        edge_indices, edge_attrs = [], []\n        for bond in mol.GetBonds():\n            i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n            edge_indices.extend([(i, j), (j, i)])\n            bond_type = bond.GetBondTypeAsDouble()\n            edge_attrs.extend([[bond_type], [bond_type]])\n\n        if not edge_indices:\n            edge_index = torch.empty((2, 0), dtype=torch.long)\n            edge_attr = torch.empty((0, 1), dtype=torch.float)\n        else:\n            edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n            edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n\n        if y_val is not None:\n            y_tensor = torch.tensor([[y_val]], dtype=torch.float)\n            return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y_tensor)\n        else:\n            return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n    except Exception as e:\n        return None\n\nfrom rdkit.Chem import Descriptors\n\n# A dictionary mapping labels to their most important global features from XGBoost\nLABEL_SPECIFIC_FEATURES = {\n    'Tg': [\n        \"HallKierAlpha\", # Topological charge index\n        \"MolLogP\",       # Lipophilicity\n        \"NumRotatableBonds\", # Flexibility\n        \"TPSA\",          # Polarity\n    ],\n    'FFV': [\n        \"NHOHCount\",     # Count of NH and OH groups (H-bonding)\n        \"NumRotatableBonds\",\n        \"MolWt\",         # Size\n        \"TPSA\",\n    ],\n    'Tc': [\n        \"MolLogP\",\n        \"NumValenceElectrons\",\n        \"SPS\",           # Molecular shape index\n        \"MolWt\",\n    ],\n    'Density': [\n        \"MolWt\",\n        \"MolMR\",         # Molar refractivity (related to volume)\n        \"FractionCSP3\",  # Proportion of sp3 hybridized carbons (related to saturation)\n        \"NumHeteroatoms\",\n    ],\n    'Rg': [\n        \"HallKierAlpha\",\n        \"MolWt\",\n        \"NumValenceElectrons\",\n        \"qed\",           # Quantitative Estimation of Drug-likeness\n    ]\n}\n\n# A helper dictionary to easily call RDKit functions from their string names\nRDKIT_DESC_CALCULATORS = {name: func for name, func in Descriptors.descList}\nRDKIT_DESC_CALCULATORS['qed'] = Descriptors.qed # Add qed as it's not in the default list\n\nfrom rdkit import Chem\nimport numpy as np\n\n# This ATOM_MAP dictionary must be defined globally in your script (it already is)\n# ATOM_MAP = {'C': 0, 'N': 1, ...}\n\ndef smiles_to_graph_label_specific(smiles_str: str, label: str, y_val=None):\n    \"\"\"\n    (BASELINE VERSION - SIMPLE FEATURES)\n    - This is the original hybrid GNN featurizer that produced your best score.\n    - Node Features (x): Atom one-hot (20) + 5 atom features = 25 features.\n    - Edge Features (edge_attr): Bond type as double = 1 feature.\n    - Global Features (u): Label-specific descriptors are stored separately in 'data.u'.\n    \"\"\"\n    try:\n        mol = Chem.MolFromSmiles(smiles_str)\n        if mol is None: \n            return None\n\n        # --- 1. Calculate and store label-specific GLOBAL features ---\n        global_features = []\n        features_to_calculate = LABEL_SPECIFIC_FEATURES.get(label, [])\n        \n        for feature_name in features_to_calculate:\n            calculator_func = RDKIT_DESC_CALCULATORS.get(feature_name)\n            if calculator_func:\n                try:\n                    val = calculator_func(mol)\n                    # Ensure value is valid, replace inf/nan with 0\n                    global_features.append(val if np.isfinite(val) else 0.0)\n                except Exception as e:\n                    global_features.append(0.0)\n            else:\n                global_features.append(0.0)\n\n        # --- 2. Create Node Features (SIMPLE) ---\n        node_features = []\n        for atom in mol.GetAtoms():\n            # One-Hot Symbol (len 20, from global ATOM_MAP)\n            atom_features = [0] * len(ATOM_MAP)\n            symbol = atom.GetSymbol()\n            if symbol in ATOM_MAP:\n                atom_features[ATOM_MAP[symbol]] = 1\n\n            # Standard Features (len 5)\n            atom_features.extend([\n                atom.GetAtomicNum(),\n                atom.GetTotalDegree(),\n                atom.GetFormalCharge(),\n                atom.GetTotalNumHs(),\n                int(atom.GetIsAromatic())\n            ])\n            # Total features = 25\n            node_features.append(atom_features)\n        \n        if not node_features: return None\n        x = torch.tensor(node_features, dtype=torch.float)\n\n        # --- 3. Create Edge Features (SIMPLE) ---\n        edge_indices, edge_attrs = [], []\n        for bond in mol.GetBonds():\n            i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n            edge_indices.extend([(i, j), (j, i)])\n            bond_type = bond.GetBondTypeAsDouble()\n            edge_attrs.extend([[bond_type], [bond_type]]) # 1-dim feature\n\n        if not edge_indices:\n            edge_index = torch.empty((2, 0), dtype=torch.long)\n            edge_attr = torch.empty((0, 1), dtype=torch.float) # Shape (0, 1)\n        else:\n            edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n            edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n\n        # --- 4. Create Data Object ---\n        data_obj = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n        data_obj.u = torch.tensor([global_features], dtype=torch.float) # Store globals in 'u'\n\n        if y_val is not None:\n            data_obj.y = torch.tensor([[y_val]], dtype=torch.float)\n        \n        return data_obj\n        \n    except Exception as e:\n        # Catch any other unexpected molecule-level errors\n        print(f\"CRITICAL ERROR converting SMILES '{smiles_str}': {e}\")\n        return None\n            \nclass GNNModel(torch.nn.Module):\n    \"\"\"\n    Defines the Graph Neural Network architecture.\n    \"\"\"\n    def __init__(self, num_node_features, hidden_channels=128):\n        super(GNNModel, self).__init__()\n        torch.manual_seed(42)\n        \n        self.conv1 = GCNConv(num_node_features, hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels * 2)\n        self.conv3 = GCNConv(hidden_channels * 2, hidden_channels * 4)\n        self.lin = torch.nn.Linear(hidden_channels * 4, 1)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        \n        x = F.relu(self.conv1(x, edge_index))\n        x = F.relu(self.conv2(x, edge_index))\n        x = self.conv3(x, edge_index)\n        x = global_max_pool(x, batch) # Aggregate node features to get a graph-level embedding\n        x = F.dropout(x, p=0.25, training=self.training)\n        x = self.lin(x)\n        \n        return x\n\n\ndef predict_with_gnn(trained_model, test_smiles):\n    \"\"\"\n    Uses a pre-trained GNN model to make predictions on a list of test SMILES.\n    \"\"\"\n    if trained_model is None:\n        print(\"Prediction skipped because the GNN model is invalid.\")\n        return np.full(len(test_smiles), np.nan)\n\n    print(\"--- Making predictions with trained GNN... ---\")\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # Convert test SMILES to graph data\n    test_data_list = [smiles_to_graph(s) for s in test_smiles]\n    \n    # We need to keep track of which original indices are valid\n    valid_indices = [i for i, data in enumerate(test_data_list) if data is not None]\n    valid_test_data = [data for data in test_data_list if data is not None]\n\n    if not valid_test_data:\n        print(\"Warning: No valid test molecules could be converted to graphs.\")\n        return np.full(len(test_smiles), np.nan)\n        \n    test_loader = PyGDataLoader(valid_test_data, batch_size=32, shuffle=False)\n\n    trained_model.eval()\n    all_preds = []\n    with torch.no_grad():\n        for data in test_loader:\n            data = data.to(DEVICE)\n            out = trained_model(data)\n            all_preds.append(out.cpu())\n\n    # Combine predictions from all batches\n    test_preds_tensor = torch.cat(all_preds, dim=0).numpy().flatten()\n    \n    # Create a full-sized prediction array and fill in the values at their original positions\n    final_predictions = np.full(len(test_smiles), np.nan)\n    if len(test_preds_tensor) == len(valid_indices):\n        final_predictions[valid_indices] = test_preds_tensor\n    else:\n        print(f\"Warning: Mismatch in GNN prediction count. This can happen with invalid SMILES.\")\n        fill_count = min(len(valid_indices), len(test_preds_tensor))\n        final_predictions[valid_indices[:fill_count]] = test_preds_tensor[:fill_count]\n\n    return final_predictions\n\nimport json\nimport os\n\ndef save_gnn_model(model, label, model_dir=\"models/gnn\"):\n    \"\"\"\n    (MODIFIED) Saves the GNN model state_dict and its full constructor config.\n    \"\"\"\n    if model is None:\n        print(f\"Skipping save for {label}, model is None.\")\n        return\n\n    os.makedirs(model_dir, exist_ok=True)\n    model_path = os.path.join(model_dir, f\"gnn_model_{label}.pth\")\n    config_path = os.path.join(model_dir, f\"gnn_config_{label}.json\")\n\n    # Save the model parameters (the weights)\n    torch.save(model.state_dict(), model_path)\n    \n    # Save the full configuration dictionary\n    with open(config_path, 'w') as f:\n        json.dump(model.config_args, f, indent=4)\n        \n    print(f\"Saved final model for {label} to {model_path}\")\n\n\ndef load_gnn_model(label, model_dir=\"models/gnn\"):\n    \"\"\"\n    (MODIFIED) Loads a saved GNN model using its full config file.\n    \"\"\"\n    model_path = os.path.join(model_dir, f\"gnn_model_{label}.pth\")\n    config_path = os.path.join(model_dir, f\"gnn_config_{label}.json\")\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    if not os.path.exists(model_path) or not os.path.exists(config_path):\n        print(f\"Warning: Model or config file not found for {label}. Cannot load model.\")\n        return None\n\n    with open(config_path, 'r') as f:\n        config = json.load(f)\n    \n    try:\n        # Re-initialize the model using all saved config args via dictionary unpacking\n        model = TaskSpecificGNN(**config).to(DEVICE)\n        \n        # Load the saved model weights\n        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n        model.eval() # Set model to evaluation mode\n        print(f\"Successfully loaded saved model for {label} from {model_path}\")\n        return model\n\n    except Exception as e:\n        print(f\"CRITICAL ERROR loading model for {label}: {e}\")\n        print(\"This may be due to a mismatch between the saved model and the current model class definition.\")\n        return None\n    \n\ndef create_dynamic_mlp(input_dim, layer_list, dropout_list):\n    \"\"\"\n    Helper function to dynamically build the task-specific MLP.\n    \"\"\"\n    layers = []\n    current_dim = input_dim\n    \n    for neurons, dropout in zip(layer_list, dropout_list):\n        layers.append(torch.nn.Linear(current_dim, neurons))\n        layers.append(torch.nn.ReLU())\n        layers.append(torch.nn.Dropout(dropout))\n        current_dim = neurons\n        \n    # Add the final single-output prediction layer\n    layers.append(torch.nn.Linear(current_dim, 1))\n    \n    return torch.nn.Sequential(*layers)\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GATConv, global_mean_pool\n\nclass TaskSpecificGNN(torch.nn.Module):\n    def __init__(self, num_node_features, num_edge_features, num_global_features,\n                 hidden_channels_gnn, mlp_neurons, mlp_dropouts, heads=8):\n        super().__init__()\n        torch.manual_seed(42)\n\n        # --- 1. GNN Backbone (Using GATConv, No BatchNorm) ---\n        self.convs = torch.nn.ModuleList()\n\n        # Layer 1\n        self.convs.append(\n            GATConv(num_node_features, hidden_channels_gnn, heads=heads,\n                    edge_dim=num_edge_features)\n        )\n\n        # Layer 2\n        self.convs.append(\n            GATConv(hidden_channels_gnn * heads, hidden_channels_gnn * 2, heads=heads,\n                    edge_dim=num_edge_features)\n        )\n\n        # Layer 3 (Final GNN layer)\n        self.convs.append(\n            GATConv(hidden_channels_gnn * 2 * heads, hidden_channels_gnn * 4, heads=heads,\n                    concat=False, edge_dim=num_edge_features)\n        )\n\n        gnn_output_dim = hidden_channels_gnn * 4\n\n        # --- 2. Readout Head ---\n        combined_feature_size = gnn_output_dim + num_global_features\n\n        self.readout_mlp = create_dynamic_mlp(\n            input_dim=combined_feature_size,\n            layer_list=mlp_neurons,\n            dropout_list=mlp_dropouts\n        )\n\n        # --- 3. Store config for saving/loading ---\n        self.config_args = {\n            'num_node_features': num_node_features,\n            'num_edge_features': num_edge_features,\n            'num_global_features': num_global_features,\n            'hidden_channels_gnn': hidden_channels_gnn,\n            'mlp_neurons': mlp_neurons,\n            'mlp_dropouts': mlp_dropouts,\n            'heads': heads\n        }\n\n    def forward(self, data):\n        x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch\n\n        # GNN Layers with ReLU and Dropout\n        x = F.relu(self.convs[0](x, edge_index, edge_attr))\n        x = F.dropout(x, p=0.5, training=self.training)\n\n        x = F.relu(self.convs[1](x, edge_index, edge_attr))\n        x = F.dropout(x, p=0.5, training=self.training)\n\n        x = F.relu(self.convs[2](x, edge_index, edge_attr))\n\n        # Readout\n        graph_embedding = global_mean_pool(x, batch)\n        combined_features = torch.cat([graph_embedding, u], dim=1)\n        output = self.readout_mlp(combined_features)\n\n        return output\n        \n# This is a new helper, just to make scaling code cleaner inside the loops\ndef scale_graph_features(data_list, u_scaler, x_scaler, atom_map_len):\n    \"\"\"Applies fitted scalers in-place to a list of Data objects.\"\"\"\n    try:\n        for data in data_list:\n            # 1. Scale global features (u)\n            data.u = torch.tensor(u_scaler.transform(data.u.numpy()), dtype=torch.float)\n            \n            # 2. Scale continuous part of node features (x)\n            x_one_hot = data.x[:, :atom_map_len]\n            x_continuous = data.x[:, atom_map_len:]\n            \n            x_continuous_scaled = x_scaler.transform(x_continuous.numpy())\n            x_continuous_scaled_tensor = torch.tensor(x_continuous_scaled, dtype=torch.float)\n            \n            # Recombine scaled features\n            data.x = torch.cat([x_one_hot, x_continuous_scaled_tensor], dim=1)\n            \n    except Exception as e:\n        print(f\"CRITICAL ERROR applying scalers: {e}. Check feature dimensions. AtomMapLen={atom_map_len}\")\n        raise e\n    return data_list\n\n\ndef train_gnn_model(label, train_data_list, val_data_list, mlp_neurons, mlp_dropouts, epochs=300): # Increased default epochs\n    \"\"\"\n    (REVISED)\n    - Accepts both train and val data lists.\n    - Implements ReduceLROnPlateau scheduler based on val_loss.\n    - Implements Early Stopping based on val_loss patience.\n    \"\"\"\n    print(f\"--- Training GNN for label: {label} ---\")\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    if not train_data_list:\n        print(f\"Warning: Empty train data list passed for {label}.\")\n        return None\n    if not val_data_list:\n        print(f\"Warning: Empty validation data list passed for {label}.\")\n        return None\n\n    # drop_last=True is important for training stability, prevents variance from tiny final batches.\n    train_loader = PyGDataLoader(train_data_list, batch_size=32, shuffle=True, drop_last=True) \n    val_loader = PyGDataLoader(val_data_list, batch_size=32, shuffle=False) # No shuffle/drop for val\n\n    # Get feature dimensions from the first data object\n    first_data = train_data_list[0]\n    num_node_features = first_data.x.shape[1]\n    num_global_features = first_data.u.shape[1]\n    num_edge_features = first_data.edge_attr.shape[1]\n    \n    print(f\"Model Features (Scaled): Nodes={num_node_features}, Edges={num_edge_features}, Global={num_global_features}\")\n\n    model = TaskSpecificGNN(  # This should be your (no-BN) model class\n        num_node_features=num_node_features,\n        num_edge_features=num_edge_features,\n        num_global_features=num_global_features,\n        hidden_channels_gnn=128, \n        mlp_neurons=mlp_neurons,\n        mlp_dropouts=mlp_dropouts\n    ).to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    criterion = torch.nn.L1Loss() \n\n    # --- 1. ADD SCHEDULER ---\n    # This will cut the LR by half (factor=0.5) if val loss doesn't improve for 10 epochs (patience=10)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n\n    # --- 2. ADD EARLY STOPPING VARS ---\n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n    PATIENCE_EPOCHS = 30  # Stop training if val loss doesn't improve for 30 straight epochs\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        total_train_loss = 0\n        for data in train_loader:\n            if data.x.shape[0] <= 1: # Skip batches with one node (can happen)\n                continue\n            data = data.to(DEVICE)\n            optimizer.zero_grad()\n            out = model(data)\n            loss = criterion(out, data.y)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            total_train_loss += loss.item() * data.num_graphs\n        \n        if len(train_loader.dataset) == 0:\n            avg_train_loss = 0\n        else:\n            avg_train_loss = total_train_loss / len(train_loader.dataset)\n\n        # --- 3. ADD VALIDATION LOOP (INSIDE EPOCH LOOP) ---\n        model.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for data in val_loader:\n                data = data.to(DEVICE)\n                out = model(data)\n                loss = criterion(out, data.y)\n                total_val_loss += loss.item() * data.num_graphs\n        \n        if len(val_loader.dataset) == 0:\n             avg_val_loss = 0\n        else:\n            avg_val_loss = total_val_loss / len(val_loader.dataset)\n\n        if epoch % 10 == 0 or epoch == 1:\n             print(f\"Epoch: {epoch:03d}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n\n        # --- 4. SCHEDULER & EARLY STOPPING LOGIC ---\n        scheduler.step(avg_val_loss) # Feed validation loss to the scheduler\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n\n        if epochs_no_improve >= PATIENCE_EPOCHS and epoch > 50: # Give it at least 50 epochs to warm up\n            print(f\"--- Early stopping triggered at epoch {epoch} ---\")\n            break\n            \n    print(f\"--- GNN training for {label} complete. Best Val Loss: {best_val_loss:.6f} ---\")\n    return model\n\ndef predict_with_gnn(trained_model, test_smiles, label, u_scaler, x_scaler, atom_map_len):\n    \"\"\"\n    (MODIFIED for Full Scaling)\n    - Requires both u_scaler (global) and x_scaler (node) to transform features.\n    - Returns SCALED predictions.\n    \"\"\"\n    if trained_model is None or u_scaler is None or x_scaler is None:\n        print(f\"Prediction skipped for {label} due to missing model or scaler.\")\n        return np.full(len(test_smiles), np.nan)\n\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # 1. Featurize test data (features are NOT scaled yet)\n    test_data_list = [smiles_to_graph_label_specific(s, label, y_val=None) for s in test_smiles]\n    \n    valid_indices = [i for i, data in enumerate(test_data_list) if data is not None]\n    valid_test_data = [data for data in test_data_list if data is not None]\n\n    if not valid_test_data:\n        print(f\"Warning: No valid test molecules could be converted for {label}.\")\n        return np.full(len(test_smiles), np.nan)\n        \n    # 2. Apply fitted scalers to all valid test features\n    try:\n        valid_test_data = scale_graph_features(valid_test_data, u_scaler, x_scaler, atom_map_len)\n    except Exception as e:\n        print(f\"CRITICAL ERROR applying scalers during prediction: {e}.\")\n        return np.full(len(test_smiles), np.nan)\n\n    test_loader = PyGDataLoader(valid_test_data, batch_size=32, shuffle=False) \n\n    trained_model.eval()\n    all_preds = []\n    with torch.no_grad():\n        for data in test_loader:\n            data = data.to(DEVICE)\n            out = trained_model(data)\n            all_preds.append(out.cpu())\n\n    test_preds_tensor = torch.cat(all_preds, dim=0).numpy().flatten()\n    \n    # Fill predictions array (these are SCALED predictions)\n    final_predictions = np.full(len(test_smiles), np.nan)\n    if len(test_preds_tensor) == len(valid_indices):\n        final_predictions[valid_indices] = test_preds_tensor\n    else:\n        print(f\"Warning: Mismatch in GNN prediction count for {label}.\")\n        fill_count = min(len(valid_indices), len(test_preds_tensor))\n        final_predictions[valid_indices[:fill_count]] = test_preds_tensor[:fill_count]\n\n    return final_predictions # These predictions are on the SCALED range\n\n\ndef train_or_predict_gnn(train_model=True, model_dir=\"models/gnn\", n_splits=10):\n    \"\"\"\n    (FINAL COMPLETE VERSION)\n    - All data hardening (coerce, filter) and RobustScaler logic is included.\n    - CV loop is modified to create a val_data_list.\n    - Calls the new, optimized train_gnn_model with scheduler/early stopping.\n    - Correctly passes all arguments (config['neurons'], config['dropouts']) to fix the TypeError.\n    \"\"\"\n    \n    ATOM_MAP_LEN = 20  # Make sure this matches your global ATOM_MAP\n    \n    # Plausible physical ranges to filter catastrophic outliers BEFORE scaling\n    VALID_RANGES = {\n        'Tg':      (-100, 500),  \n        'FFV':     (0.01, 0.99), \n        'Tc':      (0, 1000),    \n        'Density': (0.1, 3.0),   \n        'Rg':      (0.1, 200)    \n    }\n\n    # MLP configs for the GNN readout head\n    best_configs = {\n        # Classic funnel, slightly lower final dropout\n        \"Tg\":      {\"neurons\": [512, 256, 128], \"dropouts\": [0.5, 0.4, 0.2]},\n        # Original wide funnel for this complex feature\n        \"Density\": {\"neurons\": [1024, 256, 64], \"dropouts\": [0.5, 0.4, 0.3]},\n        # Even wider and deeper, with strong regularization for presumed complexity\n        \"FFV\":     {\"neurons\": [1024, 512, 64], \"dropouts\": [0.6, 0.5, 0.4]},\n        # Slightly deeper than the simplest model to capture more features\n        \"Tc\":      {\"neurons\": [128, 64], \"dropouts\": [0.4, 0.3]},\n        # A gentle funnel instead of a pure block to encourage feature compression\n        \"Rg\":      {\"neurons\": [128, 64, 64], \"dropouts\": [0.4, 0.3, 0.3]},\n    }\n    default_config = {\"neurons\": [128, 64], \"dropouts\": [0.3, 0.3]}\n\n    output_df = pd.DataFrame({'id': test_df['id']})\n    cv_mae_results = []\n    os.makedirs(model_dir, exist_ok=True)\n    warnings.filterwarnings(\"ignore\", \"Mean of empty slice\", RuntimeWarning)\n\n    for label in labels: \n        print(f\"\\n{'='*20} Processing GNN for label: {label} {'='*20}\")\n        \n        config = best_configs.get(label, default_config)\n        print(f\"Using MLP Config: Neurons={config['neurons']}, Dropouts={config['dropouts']}\")\n        \n        ensemble_models = []\n        y_scaler_path = os.path.join(model_dir, f\"gnn_yscaler_{label}.joblib\")\n        u_scaler_path = os.path.join(model_dir, f\"gnn_uscaler_{label}.joblib\")\n        x_scaler_path = os.path.join(model_dir, f\"gnn_xscaler_{label}.joblib\")\n        \n        if train_model:\n            # --- START DATA HARDENING ---\n            all_smiles_raw = subtables[label]['SMILES']\n            all_y_raw = subtables[label][label] \n            \n            all_y_numeric = pd.to_numeric(all_y_raw, errors='coerce')\n            original_count = len(all_y_numeric)\n\n            valid_min, valid_max = VALID_RANGES.get(label, (-np.inf, np.inf))\n            valid_mask = (all_y_numeric >= valid_min) & (all_y_numeric <= valid_max) & (all_y_numeric.notna())\n            \n            all_y = all_y_numeric[valid_mask].reset_index(drop=True)\n            all_smiles = all_smiles_raw[valid_mask].reset_index(drop=True)\n            \n            print(f\"FILTERING: Coerced {original_count} rows. Kept {len(all_y)} valid rows within range ({valid_min}, {valid_max}).\")\n            \n            if len(all_y) < (2 * n_splits): \n                print(f\"CRITICAL: Not enough valid data ({len(all_y)}) to train for {label} with {n_splits} splits. Skipping.\")\n                continue\n            # --- END DATA HARDENING ---\n\n            # --- 1. FIT Y-SCALER (ROBUST) ---\n            print(\"Using RobustScaler for Y-Scaler.\")\n            y_scaler = RobustScaler()  \n            all_y_scaled = y_scaler.fit_transform(all_y.values.reshape(-1, 1)).flatten()\n            joblib.dump(y_scaler, y_scaler_path)\n            print(f\"Saved Y-Scaler for {label}\")\n\n            # --- 2. FIT INPUT SCALERS (ROBUST) ---\n            print(\"Pre-computing all graph features to fit input scalers...\")\n            all_train_graphs_raw = [smiles_to_graph_label_specific(s, label, None) for s in all_smiles]\n            \n            # Sync graph list with all data (skipping any SMILES that fail featurization)\n            all_train_graphs_synced = []\n            all_y_scaled_synced = [] \n            all_y_original_synced = [] # Also sync original Y for the CV split\n            all_smiles_synced = []     # Also sync SMILES for the CV split\n            \n            for i, graph in enumerate(all_train_graphs_raw):\n                if graph is not None:\n                    all_train_graphs_synced.append(graph)\n                    all_y_scaled_synced.append(all_y_scaled[i]) \n                    all_y_original_synced.append(all_y[i]) # Keep the original, unscaled, clean Y\n                    all_smiles_synced.append(all_smiles[i]) # Keep the matching SMILES\n            \n            all_train_graphs = all_train_graphs_synced \n            all_y_scaled = np.array(all_y_scaled_synced)\n            all_y_original_df = pd.Series(all_y_original_synced) # Store as Series for .iloc\n            all_smiles_df = pd.Series(all_smiles_synced)         # Store as Series for .iloc\n\n            if not all_train_graphs:\n                print(f\"CRITICAL: No valid training graphs could be featurized for {label}. Skipping.\")\n                continue\n                \n            all_u_data = np.concatenate([d.u.numpy() for d in all_train_graphs], axis=0)\n            print(\"Using RobustScaler for U-Scaler.\")\n            u_scaler = RobustScaler().fit(all_u_data)  # Use RobustScaler\n            joblib.dump(u_scaler, u_scaler_path)\n            print(f\"Saved U-Scaler for {label}\")\n\n            all_x_data = torch.cat([d.x for d in all_train_graphs], dim=0)\n            all_x_continuous = all_x_data[:, ATOM_MAP_LEN:].numpy()\n            print(\"Using RobustScaler for X-Scaler.\")\n            x_scaler = RobustScaler().fit(all_x_continuous)  # Use RobustScaler\n            joblib.dump(x_scaler, x_scaler_path)\n            print(f\"Saved X-Scaler for {label}\")\n\n            # --- 3. APPLY SCALERS ---\n            all_data_objects_scaled = scale_graph_features(all_train_graphs, u_scaler, x_scaler, ATOM_MAP_LEN)\n            for i, data_obj in enumerate(all_data_objects_scaled):\n                data_obj.y = torch.tensor([[all_y_scaled[i]]], dtype=torch.float)\n            \n            # --- 4. K-FOLD CV LOOP (MODIFIED) ---\n            kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n            fold_val_scores = []\n            fold_indices_gen = kf.split(all_data_objects_scaled) # Split the synced, valid, scaled data\n\n            for fold, (train_idx, val_idx) in enumerate(fold_indices_gen):\n                print(f\"\\n--- Fold {fold+1}/{n_splits} for {label} ---\")\n                \n                train_data_list = [all_data_objects_scaled[i] for i in train_idx]\n                val_data_list = [all_data_objects_scaled[i] for i in val_idx] # <-- CREATE VAL LIST\n                \n                val_smiles_list = all_smiles_df.iloc[val_idx].tolist()\n                y_val_original = all_y_original_df.iloc[val_idx].values \n\n                fold_model = train_gnn_model(\n                    label,\n                    train_data_list, # Pass train data\n                    val_data_list,   # <-- Pass val data\n                    config['neurons'],    # <-- PASSES mlp_neurons\n                    config['dropouts'],   # <-- PASSES mlp_dropouts (FIXES ERROR)\n                    epochs=300       # <-- Train longer (will stop early)\n                )\n                \n                if fold_model:\n                    print(\"Running final validation prediction on the best model...\")\n                    val_preds_scaled = predict_with_gnn(fold_model, val_smiles_list, label, u_scaler, x_scaler, ATOM_MAP_LEN)\n                    \n                    train_y_scaled_median = 0.0 # RobustScaler median is 0\n                    val_preds_scaled_filled = pd.Series(val_preds_scaled).fillna(train_y_scaled_median)\n                    \n                    val_preds_original = y_scaler.inverse_transform(\n                        val_preds_scaled_filled.values.reshape(-1, 1)\n                    ).flatten()\n\n                    mae = mean_absolute_error(y_val_original, val_preds_original)\n                    print(f\"✅ Fold {fold+1} Validation MAE (Original Scale): {mae:.4f}\")\n                    fold_val_scores.append(mae)\n                    \n                    model_save_name = f\"{label}_fold{fold}\"\n                    save_gnn_model(fold_model, model_save_name, model_dir)\n                    ensemble_models.append(fold_model)\n                else:\n                    print(f\"Warning: Training failed for Fold {fold+1}. Model will be skipped.\")\n            \n            if fold_val_scores:\n                avg_cv_mae = np.mean(fold_val_scores)\n                print(f\"\\n{'*'*10} Average CV MAE for {label} (Original Scale): {avg_cv_mae:.4f} {'*'*10}\")\n                cv_mae_results.append({'label': label, 'avg_cv_mae': avg_cv_mae})\n\n        else:\n            # --- PREDICTION-ONLY MODE ---\n            print(f\"Loading {n_splits} models and ALL 3 RobustScalers for {label} ensemble...\")\n            model_path = '/kaggle/input/neurips-2025/GATConv_v29/models/gnn/'\n            try:\n                y_scaler = joblib.load(f'{model_path}gnn_yscaler_{label}.joblib')\n                u_scaler = joblib.load(f'{model_path}gnn_uscaler_{label}.joblib')\n                x_scaler = joblib.load(f'{model_path}gnn_xscaler_{label}.joblib')\n                print(\"Loaded Y, U, and X RobustScalers.\")\n            except FileNotFoundError:\n                print(f\"CRITICAL: Scaler files not found for {label}. Cannot make predictions.\")\n                continue\n\n            for fold in range(n_splits):\n                loaded_model = load_gnn_model(f\"{label}_fold{fold}\", model_path.rstrip('/'))\n                if loaded_model:\n                    ensemble_models.append(loaded_model)\n            \n            if not ensemble_models: print(f\"Warning: No models found for label {label}.\")\n            else: print(f\"Successfully loaded {len(ensemble_models)} models for ensemble.\")\n\n\n        # --- ENSEMBLE PREDICTION STEP (Test Set) ---\n        test_smiles = test_df['SMILES'].tolist()\n        \n        if ensemble_models and y_scaler and u_scaler and x_scaler:\n            print(f\"Making ensemble (scaled) predictions for {label} using {len(ensemble_models)} models...\")\n            all_fold_preds_scaled = []\n            for model in ensemble_models:\n                fold_test_preds_scaled = predict_with_gnn(model, test_smiles, label, u_scaler, x_scaler, ATOM_MAP_LEN)\n                all_fold_preds_scaled.append(fold_test_preds_scaled)\n            \n            preds_stack_scaled = np.stack(all_fold_preds_scaled)\n            final_ensemble_preds_scaled = np.nanmean(preds_stack_scaled, axis=0) \n            pred_series_scaled = pd.Series(final_ensemble_preds_scaled)\n            \n            pred_series_scaled_filled = pred_series_scaled.fillna(0.0) # Impute with scaled median (0.0)\n\n            final_preds_original = y_scaler.inverse_transform(\n                pred_series_scaled_filled.values.reshape(-1, 1)\n            ).flatten()\n            \n            output_df[label] = final_preds_original\n            \n        else:\n            print(f\"No models or scalers available for {label}. Filling with (filtered) training median.\")\n            # Robust median fallback logic\n            fallback_median = 0.0\n            try:\n                if 'all_y' in locals() and not all_y.empty:\n                     fallback_median = all_y.median()\n                else: \n                     print(\"Loading data to calculate fallback median...\")\n                     fb_y_raw = subtables[label][label]\n                     fb_y_num = pd.to_numeric(fb_y_raw, errors='coerce')\n                     valid_min, valid_max = VALID_RANGES.get(label, (-np.inf, np.inf))\n                     fb_mask = (fb_y_num >= valid_min) & (fb_y_num <= valid_max) & (fb_y_num.notna())\n                     fallback_median = fb_y_num[fb_mask].median()\n                print(f\"Using filtered median fallback: {fallback_median}\")\n            except Exception as e:\n                 print(f\"Error getting median, falling back to 0: {e}\")\n                 fallback_median = 0.0 \n                 \n            output_df[label] = fallback_median\n\n    # --- Display final CV MAE summary ---\n    if train_model and cv_mae_results:\n        print(\"\\n\" + \"=\"*40)\n        print(\"📊 HYBRID GNN 5-Fold CV MAE Summary (Original Scale):\")\n        print(\"=\"*40)\n        mae_df = pd.DataFrame(cv_mae_results)\n        print(mae_df.to_string(index=False))\n        mae_df.to_csv(\"gnn_hybrid_cv_mae_results.csv\", index=False)\n        print(\"\\nCV results saved to gnn_hybrid_cv_mae_results.csv\")\n\n    submission_path = 'submission_hybrid_gnn_final.csv'\n    output_df.to_csv(submission_path, index=False)\n    print(f\"\\n✅ GNN Ensemble predictions (Original Scale) saved to {submission_path}\")\n    \n    warnings.filterwarnings(\"default\", \"Mean of empty slice\", RuntimeWarning)\n    \n    return output_df\n\n# To train the models and then predict:\ngnn_submission_df = train_or_predict_gnn(train_model=False)\n\noutput_dfs.append(gnn_submission_df)\n\nprint(\"\\nGNN Submission Preview:\")\nprint(gnn_submission_df.head())","metadata":{"execution":{"iopub.execute_input":"2025-09-12T13:02:39.14281Z","iopub.status.busy":"2025-09-12T13:02:39.142548Z","iopub.status.idle":"2025-09-12T13:03:11.054403Z","shell.execute_reply":"2025-09-12T13:03:11.0535Z"},"papermill":{"duration":31.925797,"end_time":"2025-09-12T13:03:11.055618","exception":false,"start_time":"2025-09-12T13:02:39.129821","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(output_dfs)","metadata":{"execution":{"iopub.execute_input":"2025-09-12T13:03:11.082074Z","iopub.status.busy":"2025-09-12T13:03:11.081835Z","iopub.status.idle":"2025-09-12T13:03:11.086467Z","shell.execute_reply":"2025-09-12T13:03:11.085943Z"},"papermill":{"duration":0.018591,"end_time":"2025-09-12T13:03:11.087445","exception":false,"start_time":"2025-09-12T13:03:11.068854","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Average predictions from all output DataFrames\nfinal_df = pd.concat(output_dfs, axis=0).groupby('id').mean().reset_index()\nfinal_df.to_csv('submission.csv', index=False)\nfinal_df","metadata":{"execution":{"iopub.execute_input":"2025-09-12T13:03:11.112955Z","iopub.status.busy":"2025-09-12T13:03:11.112735Z","iopub.status.idle":"2025-09-12T13:03:11.130147Z","shell.execute_reply":"2025-09-12T13:03:11.129524Z"},"papermill":{"duration":0.031179,"end_time":"2025-09-12T13:03:11.131151","exception":false,"start_time":"2025-09-12T13:03:11.099972","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}